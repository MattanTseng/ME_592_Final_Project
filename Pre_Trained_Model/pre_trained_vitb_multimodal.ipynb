{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, PILToTensor, CenterCrop, ConvertImageDtype\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '../utils/')\n",
    "from dataset import ChestImage64\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import vit_l_16, ViT_L_16_Weights,vit_b_16, ViT_B_16_Weights\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"../256pxImages/train_labels_256p_paired.csv\"\n",
    "root_path = '../256pxImages'\n",
    "\n",
    "pairCSV = pd.read_csv(csv_path)\n",
    "pairCSV['EncodedLabels'] = ''\n",
    "print(pairCSV.shape)\n",
    "\n",
    "for i in range(4, pairCSV.shape[1]-1):\n",
    "    pairCSV['EncodedLabels'] = pairCSV['EncodedLabels'].astype(str) + pairCSV.iloc[:, i].astype(str) \n",
    "    if i < pairCSV.shape[1]-2:\n",
    "        pairCSV['EncodedLabels'] = pairCSV['EncodedLabels'].astype(str) + \",\" \n",
    "\n",
    "pairCSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test image loading\n",
    "front_file = pairCSV.iloc[0, 0]\n",
    "lat_file= pairCSV.iloc[0, 1]\n",
    "label_test = pairCSV['EncodedLabels'].iloc[0]\n",
    "\n",
    "test_path_front = os.path.join(root_path, front_file)\n",
    "test_path_lat = os.path.join(root_path, lat_file)\n",
    "\n",
    "label_test = [int(x) for x in label_test.split(\",\")]\n",
    "\n",
    "print(\"label_test: \", label_test)\n",
    "\n",
    "image_front = io.imread(test_path_front)\n",
    "print(type(image_front))\n",
    "image_front = torch.tensor(image_front)\n",
    "print(image_front.size())\n",
    "\n",
    "image_lat = io.imread(test_path_lat)\n",
    "print(type(image_lat))\n",
    "image_lat = torch.tensor(image_lat)\n",
    "print(image_lat.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the paired dataset\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, label_col, transform = None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # get the filename of the image\n",
    "        file_front = self.df.iloc[index, 0]\n",
    "        file_lat = self.df.iloc[index, 1]\n",
    "        label = self.df[self.label_col].iloc[index]\n",
    "\n",
    "        if type(label) == str:\n",
    "            label = [int(x) for x in label.split(\",\")]\n",
    "\n",
    "        # load the image from disk\n",
    "        front_path = os.path.join(self.root_dir, file_front)\n",
    "        lat_path = os.path.join(self.root_dir, file_lat)\n",
    "\n",
    "        img_front = Image.open(front_path).convert('RGB')\n",
    "        img_lat = Image.open(lat_path).convert('RGB')\n",
    "\n",
    "        label = torch.tensor(label)\n",
    "        label = label.float()\n",
    "\n",
    "        img_front = self.transform(img_front)\n",
    "        #img_front = torch.tensor(img_front)\n",
    "        #img_front = img_front.resize_((224, 224))\n",
    "        #img_front = repeat(img_front, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img_front = rearrange(img_front, \"c h w -> 1 c h w\")\n",
    "        #img_front = img_front.float()\n",
    "\n",
    "        img_lat = self.transform(img_lat)\n",
    "        #img_lat = img_lat.resize_((224, 224))\n",
    "        #img_lat = repeat(img_lat, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img_lat = rearrange(img_lat, \"c h w -> 1 c h w\")\n",
    "        #img_lat = img_lat.float()\n",
    "\n",
    "        img_pair=torch.cat((img_front,img_lat),0)\n",
    "\n",
    "\n",
    "        # if self.transform:\n",
    "            # label = self.transform(label)\n",
    "            # img = self.transform(img)\n",
    "\n",
    "        # return the image and its filename\n",
    "        return img_pair, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Compose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m testtransform \u001b[39m=\u001b[39m Compose([\n\u001b[0;32m      2\u001b[0m     PILToTensor(),\n\u001b[0;32m      3\u001b[0m     CenterCrop((\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m)),\n\u001b[0;32m      4\u001b[0m     ConvertImageDtype(torch\u001b[39m.\u001b[39mfloat32),\n\u001b[0;32m      5\u001b[0m     Normalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[0;32m      6\u001b[0m     ])\n\u001b[0;32m      7\u001b[0m pairDataset \u001b[39m=\u001b[39m PairedDataset(pairCSV, root_dir\u001b[39m=\u001b[39mroot_path, label_col\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEncodedLabels\u001b[39m\u001b[39m\"\u001b[39m, transform\u001b[39m=\u001b[39mtesttransform)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(pairDataset))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Compose' is not defined"
     ]
    }
   ],
   "source": [
    "testtransform = Compose([\n",
    "    PILToTensor(),\n",
    "    CenterCrop((224,224)),\n",
    "    ConvertImageDtype(torch.float32),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "pairDataset = PairedDataset(pairCSV, root_dir=root_path, label_col=\"EncodedLabels\", transform=testtransform)\n",
    "\n",
    "print(type(pairDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test train validate\n",
    "train_size = int(0.7 * len(pairDataset))\n",
    "val_size = int(0.1 * len(pairDataset))\n",
    "test_size = int(0.2 * len(pairDataset))\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(pairDataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(\"Train Length: \", len(train_dataset))\n",
    "print(\"Validation Length: \", len(val_dataset))\n",
    "print(\"Test Length: \", len(test_dataset))\n",
    "\n",
    "batchsize = 12\n",
    "\n",
    "# make three different dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "features, labels = next(iter(train_loader))\n",
    "print(features.size())\n",
    "print(features.dtype)\n",
    "\n",
    "print(features[1, 1, :, :])\n",
    "\n",
    "print(labels.size())\n",
    "print(datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_features=features[:, 0, :, :, :]\n",
    "print(front_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_features=features[:, 1, :, :, :]\n",
    "print(lat_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllAttentionVIT(nn.Module):\n",
    "    def __init__(self,weights_frontal,weights_lateral,code_size,num_classes,bottleneck=1024,drop_rate=0.1):\n",
    "        \n",
    "        super(AllAttentionVIT,self).__init__()\n",
    "\n",
    "        #Initialize the model\n",
    "        self.transformer_enc_frontal = vit_b_16(weights = weights_frontal)\n",
    "        self.transformer_enc_frontal.heads = nn.Sequential(nn.Linear(768, code_size))\n",
    "        self.transformer_enc_lateral = vit_b_16(weights = weights_lateral)\n",
    "        self.transformer_enc_lateral.heads = nn.Sequential(nn.Linear(768, code_size))\n",
    "        \n",
    "        self.ff1 = nn.Linear(code_size*2,num_classes)\n",
    "        #self.relu1 = nn.ReLU()\n",
    "        #self.dropout = nn.Dropout(p = drop_rate)\n",
    "        #self.ff2 = nn.Linear(bottleneck,num_classes)\n",
    "\n",
    "    #Expect two images with the same class.\n",
    "    def forward(self,x):\n",
    "\n",
    "        #Split up paired images\n",
    "        x_f=x[:, 0, :, :, :]\n",
    "        x_l=x[:, 1, :, :, :]\n",
    "        \n",
    "        #Encode front and lateral embeddings\n",
    "        x_f = self.transformer_enc_frontal(x_f)\n",
    "        x_l = self.transformer_enc_frontal(x_l)\n",
    "\n",
    "        #Concat embeddings. May want to experiment with convolutions.\n",
    "        x = torch.cat((x_f,x_l),1)\n",
    "\n",
    "        #Map concatenated embeddings onto the classes\n",
    "        x = self.ff1(x)\n",
    "        #x = self.relu1(x)\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.ff2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWeights=ViT_B_16_Weights.DEFAULT\n",
    "testModel = AllAttentionVIT(testWeights,testWeights,768,14)\n",
    "\n",
    "# simple function to determine how many TRAINABILE parameters are in the model\n",
    "def count_parameters(testModel):\n",
    "    return sum(p.numel() for p in testModel.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(testModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out a list of the parameters we are training\n",
    "for name,param in testModel.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch accumulation parameter\n",
    "target_accumulation=24\n",
    "accum_iter = target_accumulation/batchsize  \n",
    "print(accum_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's train this thing. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epochs = 20\n",
    "learning_rate = 0.0034\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# make sure the model is running on the GPU if its available\n",
    "testModel.to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "params_to_update = testModel.parameters()\n",
    "optimizer = torch.optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(model500k.parameters(), lr=learning_rate)\n",
    "\n",
    "# start a clock \n",
    "print(\"Training Starting.\")\n",
    "start_time = time.time()\n",
    "\n",
    "# create empty arrays to hold the loss results\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(epochs):\n",
    "    phase = 'train'\n",
    "    # set the model to training mode\n",
    "    testModel.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "# zero the parameter gradients at the very beginning\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # ugh. This is gross. I should have done this step at the beginning for all of the datasets. \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"Here's the size of the inputs: \", inputs.size())\n",
    "        # with torch.set_grad_enabled(phase == 'train'):\n",
    "            # run the training data through the model\n",
    "        outputs = testModel(inputs)\n",
    "\n",
    "        #calculate the loss of the model\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #Gradient accumulation\n",
    "        loss = loss\n",
    "        loss.backward()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # weights update for each gradient accumulation\n",
    "        if ((i + 1) % accum_iter == 0) or (i + 1 == len(train_loader)):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "            if ((i + 1) / accum_iter) % 500 == 0:    # record loss and test validation set every 500 gradient accumulations\n",
    "                testModel.eval()\n",
    "                v_running_loss = 0.0\n",
    "                for v, data in enumerate(val_loader):\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "                    v_outputs = testModel(inputs)\n",
    "                    v_loss = criterion(v_outputs, labels)\n",
    "                    v_running_loss += v_loss.item()\n",
    "\n",
    "                print(\"Time: \", datetime.datetime.now().strftime(\"%H:%M:%S\"), \"\\tepoch: \", epoch+1, \"batch: \", i+1, \"Training loss: \", running_loss, \"Validation loss \", v_running_loss)\n",
    "                validation_losses.append(v_running_loss)\n",
    "                training_losses.append(running_loss)\n",
    "                running_loss = 0.0\n",
    "\n",
    "    model_name= \"model-multimodal_e\" + str(epochs) + f\"_lr{learning_rate:.0e}\" + \"_bs \"+ str(batchsize)\n",
    "    model_filepath=\"../logging//\" + model_name + \"_gradacc_nodrop.pth\"\n",
    "\n",
    "    print(model_filepath)\n",
    "\n",
    "    torch.save(testModel.state_dict(), model_filepath)  \n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "train_time = end_time - start_time\n",
    "print(\"Elapsed Training Time: \", datetime.timedelta(seconds = train_time))\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(outputs))\n",
    "print(outputs.size())\n",
    "#print(outputs[1, :])\n",
    "print(outputs)\n",
    "print(labels.size())\n",
    "#print(labels[1, :])\n",
    "print(labels)\n",
    "print(outputs.dtype)\n",
    "print(labels.dtype)\n",
    "print(loss)\n",
    "\n",
    "print(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Train and Val Loss\")\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me592cuda",
   "language": "python",
   "name": "me592cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
