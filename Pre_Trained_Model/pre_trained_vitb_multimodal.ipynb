{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '../utils/')\n",
    "from dataset import ChestImage64\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import vit_l_16, ViT_L_16_Weights,vit_b_16, ViT_B_16_Weights\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30630, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Front Path</th>\n",
       "      <th>Lateral path</th>\n",
       "      <th>Patient</th>\n",
       "      <th>Study</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>EncodedLabels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frontal\\patient00002_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00002_study1_Lateral.png</td>\n",
       "      <td>patient00002</td>\n",
       "      <td>study1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1,1,0,1,1,1,0,0,1,1,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frontal\\patient00004_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00004_study1_Lateral.png</td>\n",
       "      <td>patient00004</td>\n",
       "      <td>study1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frontal\\patient00005_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00005_study1_Lateral.png</td>\n",
       "      <td>patient00005</td>\n",
       "      <td>study1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,1,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frontal\\patient00009_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00009_study1_Lateral.png</td>\n",
       "      <td>patient00009</td>\n",
       "      <td>study1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frontal\\patient00010_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00010_study1_Lateral.png</td>\n",
       "      <td>patient00010</td>\n",
       "      <td>study1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Front Path  \\\n",
       "0  Frontal\\patient00002_study1_Frontal.png   \n",
       "1  Frontal\\patient00004_study1_Frontal.png   \n",
       "2  Frontal\\patient00005_study1_Frontal.png   \n",
       "3  Frontal\\patient00009_study1_Frontal.png   \n",
       "4  Frontal\\patient00010_study1_Frontal.png   \n",
       "\n",
       "                              Lateral path       Patient   Study  \\\n",
       "0  Lateral\\patient00002_study1_Lateral.png  patient00002  study1   \n",
       "1  Lateral\\patient00004_study1_Lateral.png  patient00004  study1   \n",
       "2  Lateral\\patient00005_study1_Lateral.png  patient00005  study1   \n",
       "3  Lateral\\patient00009_study1_Lateral.png  patient00009  study1   \n",
       "4  Lateral\\patient00010_study1_Lateral.png  patient00010  study1   \n",
       "\n",
       "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
       "0                           1             1             1            1      0   \n",
       "1                           0             0             0            0      0   \n",
       "2                           0             0             0            0      0   \n",
       "3                           1             1             0            0      0   \n",
       "4                           0             0             0            0      0   \n",
       "\n",
       "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
       "0              1          1            1             0                 0   \n",
       "1              0          0            0             0                 0   \n",
       "2              0          0            0             0                 0   \n",
       "3              0          0            0             0                 0   \n",
       "4              0          0            0             0                 0   \n",
       "\n",
       "   Pleural Other  Fracture  Support Devices  No Finding  \\\n",
       "0              1         1                0           0   \n",
       "1              0         0                0           1   \n",
       "2              0         0                1           0   \n",
       "3              0         0                0           0   \n",
       "4              0         0                0           1   \n",
       "\n",
       "                 EncodedLabels  \n",
       "0  1,1,1,1,0,1,1,1,0,0,1,1,0,0  \n",
       "1  0,0,0,0,0,0,0,0,0,0,0,0,0,1  \n",
       "2  0,0,0,0,0,0,0,0,0,0,0,0,1,0  \n",
       "3  1,1,0,0,0,0,0,0,0,0,0,0,0,0  \n",
       "4  0,0,0,0,0,0,0,0,0,0,0,0,0,1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"../256pxImages/train_labels_256p_paired.csv\"\n",
    "root_path = '../256pxImages'\n",
    "\n",
    "pairCSV = pd.read_csv(csv_path)\n",
    "pairCSV['EncodedLabels'] = ''\n",
    "print(pairCSV.shape)\n",
    "\n",
    "for i in range(4, pairCSV.shape[1]-1):\n",
    "    pairCSV['EncodedLabels'] = pairCSV['EncodedLabels'].astype(str) + pairCSV.iloc[:, i].astype(str) \n",
    "    if i < pairCSV.shape[1]-2:\n",
    "        pairCSV['EncodedLabels'] = pairCSV['EncodedLabels'].astype(str) + \",\" \n",
    "\n",
    "pairCSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_test:  [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([256, 256])\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "# test image loading\n",
    "front_file = pairCSV.iloc[0, 0]\n",
    "lat_file= pairCSV.iloc[0, 1]\n",
    "label_test = pairCSV['EncodedLabels'].iloc[0]\n",
    "\n",
    "test_path_front = os.path.join(root_path, front_file)\n",
    "test_path_lat = os.path.join(root_path, lat_file)\n",
    "\n",
    "label_test = [int(x) for x in label_test.split(\",\")]\n",
    "\n",
    "print(\"label_test: \", label_test)\n",
    "\n",
    "image_front = io.imread(test_path_front)\n",
    "print(type(image_front))\n",
    "image_front = torch.tensor(image_front)\n",
    "print(image_front.size())\n",
    "\n",
    "image_lat = io.imread(test_path_lat)\n",
    "print(type(image_lat))\n",
    "image_lat = torch.tensor(image_lat)\n",
    "print(image_lat.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the paired dataset\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, label_col, transform = None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # get the filename of the image\n",
    "        file_front = self.df.iloc[index, 0]\n",
    "        file_lat = self.df.iloc[index, 1]\n",
    "        label = self.df[self.label_col].iloc[index]\n",
    "\n",
    "        if type(label) == str:\n",
    "            label = [int(x) for x in label.split(\",\")]\n",
    "\n",
    "        # load the image from disk\n",
    "        front_path = os.path.join(self.root_dir, file_front)\n",
    "        lat_path = os.path.join(self.root_dir, file_lat)\n",
    "\n",
    "        img_front = io.imread(front_path)\n",
    "        img_lat = io.imread(lat_path)\n",
    "\n",
    "        label = torch.tensor(label)\n",
    "        label = label.float()\n",
    "\n",
    "        img_front = torch.tensor(img_front)\n",
    "        img_front = img_front.resize_((224, 224))\n",
    "        img_front = repeat(img_front, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img_front = rearrange(img_front, \"(c h) w -> 1 c h w\", c = 3)\n",
    "        img_front = img_front.float()\n",
    "\n",
    "        img_lat = torch.tensor(img_lat)\n",
    "        img_lat = img_lat.resize_((224, 224))\n",
    "        img_lat = repeat(img_lat, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img_lat = rearrange(img_lat, \"(c h) w -> 1 c h w\", c = 3)\n",
    "        img_lat = img_lat.float()\n",
    "\n",
    "        img_pair=torch.cat((img_front,img_lat),0)\n",
    "\n",
    "\n",
    "        # if self.transform:\n",
    "            # label = self.transform(label)\n",
    "            # img = self.transform(img)\n",
    "\n",
    "        # return the image and its filename\n",
    "        return img_pair, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.PairedDataset'>\n"
     ]
    }
   ],
   "source": [
    "default_transform = ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1.transforms\n",
    "pairDataset = PairedDataset(pairCSV, root_dir=root_path, label_col=\"EncodedLabels\", transform=default_transform)\n",
    "\n",
    "print(type(pairDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length:  21441\n",
      "Validation Length:  3063\n",
      "Test Length:  6126\n",
      "torch.Size([8, 2, 3, 224, 224])\n",
      "torch.float32\n",
      "tensor([[[ 74.,  56.,  41.,  ...,  60.,  50.,  45.],\n",
      "         [ 39.,  29.,  29.,  ..., 254., 254., 254.],\n",
      "         [255., 255., 255.,  ..., 254., 254., 254.],\n",
      "         ...,\n",
      "         [202., 202., 201.,  ..., 165., 171., 173.],\n",
      "         [174., 173., 173.,  ...,  47.,  49.,  50.],\n",
      "         [ 52.,  57.,  60.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        [[ 74.,  56.,  41.,  ...,  60.,  50.,  45.],\n",
      "         [ 39.,  29.,  29.,  ..., 254., 254., 254.],\n",
      "         [255., 255., 255.,  ..., 254., 254., 254.],\n",
      "         ...,\n",
      "         [202., 202., 201.,  ..., 165., 171., 173.],\n",
      "         [174., 173., 173.,  ...,  47.,  49.,  50.],\n",
      "         [ 52.,  57.,  60.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        [[ 74.,  56.,  41.,  ...,  60.,  50.,  45.],\n",
      "         [ 39.,  29.,  29.,  ..., 254., 254., 254.],\n",
      "         [255., 255., 255.,  ..., 254., 254., 254.],\n",
      "         ...,\n",
      "         [202., 202., 201.,  ..., 165., 171., 173.],\n",
      "         [174., 173., 173.,  ...,  47.,  49.,  50.],\n",
      "         [ 52.,  57.,  60.,  ...,   0.,   0.,   0.]]])\n",
      "torch.Size([8, 14])\n",
      "08:12:52\n"
     ]
    }
   ],
   "source": [
    "# split into test train validate\n",
    "train_size = int(0.7 * len(pairDataset))\n",
    "val_size = int(0.1 * len(pairDataset))\n",
    "test_size = int(0.2 * len(pairDataset))\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(pairDataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(\"Train Length: \", len(train_dataset))\n",
    "print(\"Validation Length: \", len(val_dataset))\n",
    "print(\"Test Length: \", len(test_dataset))\n",
    "\n",
    "batchsize = 8\n",
    "\n",
    "# make three different dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "features, labels = next(iter(train_loader))\n",
    "print(features.size())\n",
    "print(features.dtype)\n",
    "\n",
    "print(features[1, 1, :, :])\n",
    "\n",
    "print(labels.size())\n",
    "print(datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "front_features=features[:, 0, :, :, :]\n",
    "print(front_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "lat_features=features[:, 1, :, :, :]\n",
    "print(lat_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllAttentionVIT(nn.Module):\n",
    "    def __init__(self,weights_frontal,weights_lateral,code_size,num_classes,bottleneck=1024,drop_rate=0.1):\n",
    "        \n",
    "        super(AllAttentionVIT,self).__init__()\n",
    "\n",
    "        #Initialize the model\n",
    "        self.transformer_enc_frontal = vit_b_16(weights = weights_frontal)\n",
    "        self.transformer_enc_frontal.heads = nn.Sequential(nn.Linear(768, code_size))\n",
    "        self.transformer_enc_lateral = vit_b_16(weights = weights_lateral)\n",
    "        self.transformer_enc_lateral.heads = nn.Sequential(nn.Linear(768, code_size))\n",
    "\n",
    "        self.dropout = nn.Dropout(p = drop_rate)\n",
    "        self.ff = nn.Linear(code_size*2,num_classes)\n",
    "\n",
    "    #Expect two images with the same class.\n",
    "    def forward(self,x):\n",
    "\n",
    "        #Split up paired images\n",
    "        x_f=x[:, 0, :, :, :]\n",
    "        x_l=x[:, 1, :, :, :]\n",
    "        \n",
    "        #Encode front and lateral embeddings\n",
    "        x_f = self.transformer_enc_frontal(x_f)\n",
    "        x_l = self.transformer_enc_frontal(x_l)\n",
    "\n",
    "        #Concat embeddings. May want to experiment with convolutions.\n",
    "        x = torch.cat((x_f,x_l),1)\n",
    "\n",
    "        #Map concatenated embeddings onto the classes\n",
    "        x = self.dropout(x)\n",
    "        x = self.ff(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172800014\n"
     ]
    }
   ],
   "source": [
    "testWeights=ViT_B_16_Weights.DEFAULT\n",
    "testModel = AllAttentionVIT(testWeights,testWeights,768,14)\n",
    "\n",
    "# simple function to determine how many TRAINABILE parameters are in the model\n",
    "def count_parameters(testModel):\n",
    "    return sum(p.numel() for p in testModel.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(testModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t transformer_enc_frontal.class_token\n",
      "\t transformer_enc_frontal.conv_proj.weight\n",
      "\t transformer_enc_frontal.conv_proj.bias\n",
      "\t transformer_enc_frontal.encoder.pos_embedding\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.ln.weight\n",
      "\t transformer_enc_frontal.encoder.ln.bias\n",
      "\t transformer_enc_frontal.heads.0.weight\n",
      "\t transformer_enc_frontal.heads.0.bias\n",
      "\t transformer_enc_lateral.class_token\n",
      "\t transformer_enc_lateral.conv_proj.weight\n",
      "\t transformer_enc_lateral.conv_proj.bias\n",
      "\t transformer_enc_lateral.encoder.pos_embedding\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.ln.weight\n",
      "\t transformer_enc_lateral.encoder.ln.bias\n",
      "\t transformer_enc_lateral.heads.0.weight\n",
      "\t transformer_enc_lateral.heads.0.bias\n",
      "\t ff.weight\n",
      "\t ff.bias\n"
     ]
    }
   ],
   "source": [
    "# print out a list of the parameters we are training\n",
    "for name,param in testModel.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# batch accumulation parameter\n",
    "target_accumulation=32\n",
    "accum_iter = target_accumulation/batchsize  \n",
    "print(accum_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Starting.\n",
      "Time:  08:16:15 \tepoch:  1 batch:  200 Training loss:  29.330946415662766 Validation loss  219.29490014910698\n",
      "Time:  08:18:39 \tepoch:  1 batch:  400 Training loss:  28.51901964098215 Validation loss  217.93536752462387\n",
      "Time:  08:21:04 \tepoch:  1 batch:  600 Training loss:  28.672517254948616 Validation loss  218.5371543765068\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m# zero the parameter gradients at the very beginning\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     36\u001b[0m         inputs, labels \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     38\u001b[0m         \u001b[39m# ugh. This is gross. I should have done this step at the beginning for all of the datasets. \u001b[39;00m\n\u001b[0;32m     39\u001b[0m         \u001b[39m# zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36mPairedDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     27\u001b[0m lat_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir, file_lat)\n\u001b[0;32m     29\u001b[0m img_front \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mimread(front_path)\n\u001b[1;32m---> 30\u001b[0m img_lat \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mimread(lat_path)\n\u001b[0;32m     32\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(label)\n\u001b[0;32m     33\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\skimage\\io\\_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m     50\u001b[0m         plugin \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtifffile\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m file_or_url_context(fname) \u001b[39mas\u001b[39;00m fname:\n\u001b[1;32m---> 53\u001b[0m     img \u001b[39m=\u001b[39m call_plugin(\u001b[39m'\u001b[39m\u001b[39mimread\u001b[39m\u001b[39m'\u001b[39m, fname, plugin\u001b[39m=\u001b[39mplugin, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mplugin_args)\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(img, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\skimage\\io\\manage_plugins.py:207\u001b[0m, in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not find the plugin \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    205\u001b[0m                            (plugin, kind))\n\u001b[1;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py:15\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m@wraps\u001b[39m(imageio_imread)\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(imageio_imread(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\v2.py:200\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m imopen_args \u001b[39m=\u001b[39m decypher_format_arg(\u001b[39mformat\u001b[39m)\n\u001b[0;32m    198\u001b[0m imopen_args[\u001b[39m\"\u001b[39m\u001b[39mlegacy_mode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[39mwith\u001b[39;00m imopen(uri, \u001b[39m\"\u001b[39m\u001b[39mri\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mimopen_args) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m    201\u001b[0m     \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mread(index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\core\\imopen.py:213\u001b[0m, in \u001b[0;36mimopen\u001b[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     plugin_instance \u001b[39m=\u001b[39m candidate_plugin(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m InitializationError:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# file extension doesn't match file type\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\config\\plugins.py:107\u001b[0m, in \u001b[0;36mPluginConfig.plugin_class.<locals>.partial_legacy_plugin\u001b[1;34m(request)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpartial_legacy_plugin\u001b[39m(request):\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m LegacyPlugin(request, legacy_plugin)\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\core\\legacy_plugin_wrapper.py:78\u001b[0m, in \u001b[0;36mLegacyPlugin.__init__\u001b[1;34m(self, request, legacy_plugin)\u001b[0m\n\u001b[0;32m     72\u001b[0m source \u001b[39m=\u001b[39m (\n\u001b[0;32m     73\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m<bytes>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request\u001b[39m.\u001b[39mraw_uri, \u001b[39mbytes\u001b[39m)\n\u001b[0;32m     75\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request\u001b[39m.\u001b[39mraw_uri\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request\u001b[39m.\u001b[39mmode\u001b[39m.\u001b[39mio_mode \u001b[39m==\u001b[39m IOMode\u001b[39m.\u001b[39mread:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format\u001b[39m.\u001b[39;49mcan_read(request):\n\u001b[0;32m     79\u001b[0m         \u001b[39mraise\u001b[39;00m InitializationError(\n\u001b[0;32m     80\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m can not read `\u001b[39m\u001b[39m{\u001b[39;00msource\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m         )\n\u001b[0;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\core\\format.py:241\u001b[0m, in \u001b[0;36mFormat.can_read\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcan_read\u001b[39m(\u001b[39mself\u001b[39m, request):\n\u001b[0;32m    237\u001b[0m     \u001b[39m\"\"\"can_read(request)\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[39m    Get whether this format can read data from the specified uri.\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_can_read(request)\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\plugins\\pillow_legacy.py:269\u001b[0m, in \u001b[0;36mPillowFormat._can_read\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    267\u001b[0m factory, accept \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mOPEN[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplugin_id]\n\u001b[0;32m    268\u001b[0m \u001b[39mif\u001b[39;00m accept:\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mif\u001b[39;00m request\u001b[39m.\u001b[39;49mfirstbytes \u001b[39mand\u001b[39;00m accept(request\u001b[39m.\u001b[39mfirstbytes):\n\u001b[0;32m    270\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\core\\request.py:605\u001b[0m, in \u001b[0;36mRequest.firstbytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[39m\"\"\"The first 256 bytes of the file. These can be used to\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[39mparse the header to determine the file-format.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_firstbytes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 605\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_first_bytes()\n\u001b[0;32m    606\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_firstbytes\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\core\\request.py:625\u001b[0m, in \u001b[0;36mRequest._read_first_bytes\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    623\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[39m# Read\u001b[39;00m\n\u001b[1;32m--> 625\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_firstbytes \u001b[39m=\u001b[39m read_n_bytes(f, N)\n\u001b[0;32m    626\u001b[0m \u001b[39m# Set back\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\core\\request.py:647\u001b[0m, in \u001b[0;36mread_n_bytes\u001b[1;34m(f, N)\u001b[0m\n\u001b[0;32m    645\u001b[0m bb \u001b[39m=\u001b[39m \u001b[39mbytes\u001b[39m()\n\u001b[0;32m    646\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(bb) \u001b[39m<\u001b[39m N:\n\u001b[1;32m--> 647\u001b[0m     extra_bytes \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mread(N \u001b[39m-\u001b[39;49m \u001b[39mlen\u001b[39;49m(bb))\n\u001b[0;32m    648\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m extra_bytes:\n\u001b[0;32m    649\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now let's train this thing. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epochs = 1\n",
    "learning_rate = 0.003\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# make sure the model is running on the GPU if its available\n",
    "testModel.to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "params_to_update = testModel.parameters()\n",
    "optimizer = torch.optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(model500k.parameters(), lr=learning_rate)\n",
    "\n",
    "# start a clock \n",
    "print(\"Training Starting.\")\n",
    "start_time = time.time()\n",
    "\n",
    "# create empty arrays to hold the loss results\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(epochs):\n",
    "    phase = 'train'\n",
    "    # set the model to training mode\n",
    "    testModel.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "# zero the parameter gradients at the very beginning\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # ugh. This is gross. I should have done this step at the beginning for all of the datasets. \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"Here's the size of the inputs: \", inputs.size())\n",
    "        # with torch.set_grad_enabled(phase == 'train'):\n",
    "            # run the training data through the model\n",
    "        outputs = testModel(inputs)\n",
    "\n",
    "        #calculate the loss of the model\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #Gradient accumulation\n",
    "        loss = loss / accum_iter\n",
    "        loss.backward()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # weights update for each gradient accumulation\n",
    "        if ((i + 1) % accum_iter == 0) or (i + 1 == len(train_loader)):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            if ((i + 1) / accum_iter) % 50 == 0:    # record loss and test validation set every 50 gradient accumulations\n",
    "                testModel.eval()\n",
    "                v_running_loss = 0.0\n",
    "                for v, data in enumerate(val_loader):\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                    v_outputs = testModel(inputs)\n",
    "                    v_loss = criterion(v_outputs, labels)\n",
    "                    v_running_loss += v_loss.item()\n",
    "\n",
    "                print(\"Time: \", datetime.datetime.now().strftime(\"%H:%M:%S\"), \"\\tepoch: \", epoch+1, \"batch: \", i+1, \"Training loss: \", running_loss, \"Validation loss \", v_running_loss)\n",
    "                validation_losses.append(v_running_loss)\n",
    "                training_losses.append(running_loss)\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "train_time = end_time - start_time\n",
    "print(\"Elapsed Training Time: \", datetime.timedelta(seconds = train_time))\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(outputs))\n",
    "print(outputs.size())\n",
    "#print(outputs[1, :])\n",
    "print(outputs)\n",
    "print(labels.size())\n",
    "#print(labels[1, :])\n",
    "print(labels)\n",
    "print(outputs.dtype)\n",
    "print(labels.dtype)\n",
    "print(loss)\n",
    "\n",
    "print(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Train and Val Loss\")\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me592cuda",
   "language": "python",
   "name": "me592cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
