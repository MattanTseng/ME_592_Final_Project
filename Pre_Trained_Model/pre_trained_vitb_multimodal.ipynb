{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '../utils/')\n",
    "from dataset import ChestImage64\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import vit_l_16, ViT_L_16_Weights,vit_b_16, ViT_B_16_Weights\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30630, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Front Path</th>\n",
       "      <th>Lateral path</th>\n",
       "      <th>Patient</th>\n",
       "      <th>Study</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>EncodedLabels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frontal\\patient00002_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00002_study1_Lateral.png</td>\n",
       "      <td>patient00002</td>\n",
       "      <td>study1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1,1,0,1,1,1,0,0,1,1,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frontal\\patient00004_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00004_study1_Lateral.png</td>\n",
       "      <td>patient00004</td>\n",
       "      <td>study1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frontal\\patient00005_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00005_study1_Lateral.png</td>\n",
       "      <td>patient00005</td>\n",
       "      <td>study1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,1,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frontal\\patient00009_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00009_study1_Lateral.png</td>\n",
       "      <td>patient00009</td>\n",
       "      <td>study1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frontal\\patient00010_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00010_study1_Lateral.png</td>\n",
       "      <td>patient00010</td>\n",
       "      <td>study1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Front Path  \\\n",
       "0  Frontal\\patient00002_study1_Frontal.png   \n",
       "1  Frontal\\patient00004_study1_Frontal.png   \n",
       "2  Frontal\\patient00005_study1_Frontal.png   \n",
       "3  Frontal\\patient00009_study1_Frontal.png   \n",
       "4  Frontal\\patient00010_study1_Frontal.png   \n",
       "\n",
       "                              Lateral path       Patient   Study  \\\n",
       "0  Lateral\\patient00002_study1_Lateral.png  patient00002  study1   \n",
       "1  Lateral\\patient00004_study1_Lateral.png  patient00004  study1   \n",
       "2  Lateral\\patient00005_study1_Lateral.png  patient00005  study1   \n",
       "3  Lateral\\patient00009_study1_Lateral.png  patient00009  study1   \n",
       "4  Lateral\\patient00010_study1_Lateral.png  patient00010  study1   \n",
       "\n",
       "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
       "0                           1             1             1            1      0   \n",
       "1                           0             0             0            0      0   \n",
       "2                           0             0             0            0      0   \n",
       "3                           1             1             0            0      0   \n",
       "4                           0             0             0            0      0   \n",
       "\n",
       "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
       "0              1          1            1             0                 0   \n",
       "1              0          0            0             0                 0   \n",
       "2              0          0            0             0                 0   \n",
       "3              0          0            0             0                 0   \n",
       "4              0          0            0             0                 0   \n",
       "\n",
       "   Pleural Other  Fracture  Support Devices  No Finding  \\\n",
       "0              1         1                0           0   \n",
       "1              0         0                0           1   \n",
       "2              0         0                1           0   \n",
       "3              0         0                0           0   \n",
       "4              0         0                0           1   \n",
       "\n",
       "                 EncodedLabels  \n",
       "0  1,1,1,1,0,1,1,1,0,0,1,1,0,0  \n",
       "1  0,0,0,0,0,0,0,0,0,0,0,0,0,1  \n",
       "2  0,0,0,0,0,0,0,0,0,0,0,0,1,0  \n",
       "3  1,1,0,0,0,0,0,0,0,0,0,0,0,0  \n",
       "4  0,0,0,0,0,0,0,0,0,0,0,0,0,1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"../256pxImages/train_labels_256p_paired.csv\"\n",
    "root_path = '../256pxImages'\n",
    "\n",
    "pairCSV = pd.read_csv(csv_path)\n",
    "pairCSV['EncodedLabels'] = ''\n",
    "print(pairCSV.shape)\n",
    "\n",
    "for i in range(4, pairCSV.shape[1]-1):\n",
    "    pairCSV['EncodedLabels'] = pairCSV['EncodedLabels'].astype(str) + pairCSV.iloc[:, i].astype(str) \n",
    "    if i < pairCSV.shape[1]-2:\n",
    "        pairCSV['EncodedLabels'] = pairCSV['EncodedLabels'].astype(str) + \",\" \n",
    "\n",
    "pairCSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_test:  [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([256, 256])\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "# test image loading\n",
    "front_file = pairCSV.iloc[0, 0]\n",
    "lat_file= pairCSV.iloc[0, 1]\n",
    "label_test = pairCSV['EncodedLabels'].iloc[0]\n",
    "\n",
    "test_path_front = os.path.join(root_path, front_file)\n",
    "test_path_lat = os.path.join(root_path, lat_file)\n",
    "\n",
    "label_test = [int(x) for x in label_test.split(\",\")]\n",
    "\n",
    "print(\"label_test: \", label_test)\n",
    "\n",
    "image_front = io.imread(test_path_front)\n",
    "print(type(image_front))\n",
    "image_front = torch.tensor(image_front)\n",
    "print(image_front.size())\n",
    "\n",
    "image_lat = io.imread(test_path_lat)\n",
    "print(type(image_lat))\n",
    "image_lat = torch.tensor(image_lat)\n",
    "print(image_lat.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the paired dataset\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, label_col, transform = None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # get the filename of the image\n",
    "        file_front = self.df.iloc[index, 0]\n",
    "        file_lat = self.df.iloc[index, 1]\n",
    "        label = self.df[self.label_col].iloc[index]\n",
    "\n",
    "        if type(label) == str:\n",
    "            label = [int(x) for x in label.split(\",\")]\n",
    "\n",
    "        # load the image from disk\n",
    "        front_path = os.path.join(self.root_dir, file_front)\n",
    "        lat_path = os.path.join(self.root_dir, file_lat)\n",
    "\n",
    "        img_front = io.imread(front_path)\n",
    "        img_lat = io.imread(lat_path)\n",
    "\n",
    "        label = torch.tensor(label)\n",
    "        label = label.float()\n",
    "\n",
    "        img_front = torch.tensor(img_front)\n",
    "        img_front = img_front.resize_((224, 224))\n",
    "        img_front = repeat(img_front, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img_front = rearrange(img_front, \"(c h) w -> 1 c h w\", c = 3)\n",
    "        img_front = img_front.float()\n",
    "\n",
    "        img_lat = torch.tensor(img_lat)\n",
    "        img_lat = img_lat.resize_((224, 224))\n",
    "        img_lat = repeat(img_lat, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img_lat = rearrange(img_lat, \"(c h) w -> 1 c h w\", c = 3)\n",
    "        img_lat = img_lat.float()\n",
    "\n",
    "        img_pair=torch.cat((img_front,img_lat),0)\n",
    "\n",
    "\n",
    "        # if self.transform:\n",
    "            # label = self.transform(label)\n",
    "            # img = self.transform(img)\n",
    "\n",
    "        # return the image and its filename\n",
    "        return img_pair, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.PairedDataset'>\n"
     ]
    }
   ],
   "source": [
    "default_transform = ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1.transforms\n",
    "pairDataset = PairedDataset(pairCSV, root_dir=root_path, label_col=\"EncodedLabels\", transform=default_transform)\n",
    "\n",
    "print(type(pairDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length:  21441\n",
      "Validation Length:  3063\n",
      "Test Length:  6126\n",
      "torch.Size([12, 2, 3, 224, 224])\n",
      "torch.float32\n",
      "tensor([[[ 37.,  39.,  37.,  ...,  43.,  34.,  28.],\n",
      "         [ 28.,  28.,  28.,  ..., 241., 239., 239.],\n",
      "         [242., 240., 233.,  ..., 253., 253., 253.],\n",
      "         ...,\n",
      "         [184., 176., 177.,  ..., 217., 215., 215.],\n",
      "         [215., 214., 214.,  ..., 218., 220., 215.],\n",
      "         [213., 215., 213.,  ...,  32.,  34.,  39.]],\n",
      "\n",
      "        [[ 37.,  39.,  37.,  ...,  43.,  34.,  28.],\n",
      "         [ 28.,  28.,  28.,  ..., 241., 239., 239.],\n",
      "         [242., 240., 233.,  ..., 253., 253., 253.],\n",
      "         ...,\n",
      "         [184., 176., 177.,  ..., 217., 215., 215.],\n",
      "         [215., 214., 214.,  ..., 218., 220., 215.],\n",
      "         [213., 215., 213.,  ...,  32.,  34.,  39.]],\n",
      "\n",
      "        [[ 37.,  39.,  37.,  ...,  43.,  34.,  28.],\n",
      "         [ 28.,  28.,  28.,  ..., 241., 239., 239.],\n",
      "         [242., 240., 233.,  ..., 253., 253., 253.],\n",
      "         ...,\n",
      "         [184., 176., 177.,  ..., 217., 215., 215.],\n",
      "         [215., 214., 214.,  ..., 218., 220., 215.],\n",
      "         [213., 215., 213.,  ...,  32.,  34.,  39.]]])\n",
      "torch.Size([12, 14])\n",
      "23:44:53\n"
     ]
    }
   ],
   "source": [
    "# split into test train validate\n",
    "train_size = int(0.7 * len(pairDataset))\n",
    "val_size = int(0.1 * len(pairDataset))\n",
    "test_size = int(0.2 * len(pairDataset))\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(pairDataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(\"Train Length: \", len(train_dataset))\n",
    "print(\"Validation Length: \", len(val_dataset))\n",
    "print(\"Test Length: \", len(test_dataset))\n",
    "\n",
    "batchsize = 12\n",
    "\n",
    "# make three different dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "features, labels = next(iter(train_loader))\n",
    "print(features.size())\n",
    "print(features.dtype)\n",
    "\n",
    "print(features[1, 1, :, :])\n",
    "\n",
    "print(labels.size())\n",
    "print(datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "front_features=features[:, 0, :, :, :]\n",
    "print(front_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "lat_features=features[:, 1, :, :, :]\n",
    "print(lat_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllAttentionVIT(nn.Module):\n",
    "    def __init__(self,weights_frontal,weights_lateral,code_size,num_classes,bottleneck=1024,drop_rate=0.1):\n",
    "        \n",
    "        super(AllAttentionVIT,self).__init__()\n",
    "\n",
    "        #Initialize the model\n",
    "        self.transformer_enc_frontal = vit_b_16(weights = weights_frontal)\n",
    "        self.transformer_enc_frontal.heads = nn.Sequential(nn.Linear(768, code_size))\n",
    "        self.transformer_enc_lateral = vit_b_16(weights = weights_lateral)\n",
    "        self.transformer_enc_lateral.heads = nn.Sequential(nn.Linear(768, code_size))\n",
    "        \n",
    "        self.ff1 = nn.Linear(code_size*2,num_classes)\n",
    "        #self.relu1 = nn.ReLU()\n",
    "        #self.dropout = nn.Dropout(p = drop_rate)\n",
    "        #self.ff2 = nn.Linear(bottleneck,num_classes)\n",
    "\n",
    "    #Expect two images with the same class.\n",
    "    def forward(self,x):\n",
    "\n",
    "        #Split up paired images\n",
    "        x_f=x[:, 0, :, :, :]\n",
    "        x_l=x[:, 1, :, :, :]\n",
    "        \n",
    "        #Encode front and lateral embeddings\n",
    "        x_f = self.transformer_enc_frontal(x_f)\n",
    "        x_l = self.transformer_enc_frontal(x_l)\n",
    "\n",
    "        #Concat embeddings. May want to experiment with convolutions.\n",
    "        x = torch.cat((x_f,x_l),1)\n",
    "\n",
    "        #Map concatenated embeddings onto the classes\n",
    "        x = self.ff1(x)\n",
    "        #x = self.relu1(x)\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.ff2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172800014\n"
     ]
    }
   ],
   "source": [
    "testWeights=ViT_B_16_Weights.DEFAULT\n",
    "testModel = AllAttentionVIT(testWeights,testWeights,768,14)\n",
    "\n",
    "# simple function to determine how many TRAINABILE parameters are in the model\n",
    "def count_parameters(testModel):\n",
    "    return sum(p.numel() for p in testModel.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(testModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t transformer_enc_frontal.class_token\n",
      "\t transformer_enc_frontal.conv_proj.weight\n",
      "\t transformer_enc_frontal.conv_proj.bias\n",
      "\t transformer_enc_frontal.encoder.pos_embedding\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.ln.weight\n",
      "\t transformer_enc_frontal.encoder.ln.bias\n",
      "\t transformer_enc_frontal.heads.0.weight\n",
      "\t transformer_enc_frontal.heads.0.bias\n",
      "\t transformer_enc_lateral.class_token\n",
      "\t transformer_enc_lateral.conv_proj.weight\n",
      "\t transformer_enc_lateral.conv_proj.bias\n",
      "\t transformer_enc_lateral.encoder.pos_embedding\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.ln.weight\n",
      "\t transformer_enc_lateral.encoder.ln.bias\n",
      "\t transformer_enc_lateral.heads.0.weight\n",
      "\t transformer_enc_lateral.heads.0.bias\n",
      "\t ff1.weight\n",
      "\t ff1.bias\n"
     ]
    }
   ],
   "source": [
    "# print out a list of the parameters we are training\n",
    "for name,param in testModel.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# batch accumulation parameter\n",
    "target_accumulation=24\n",
    "accum_iter = target_accumulation/batchsize  \n",
    "print(accum_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Starting.\n",
      "Time:  23:54:14 \tepoch:  1 batch:  1000 Training loss:  573.4410420954227 Validation loss  146.64838421344757\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  00:10:58 \tepoch:  2 batch:  1000 Training loss:  535.3525197803974 Validation loss  139.9048591852188\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  00:27:17 \tepoch:  3 batch:  1000 Training loss:  501.0550397336483 Validation loss  126.35874739289284\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  00:43:36 \tepoch:  4 batch:  1000 Training loss:  491.1478876173496 Validation loss  123.81933218240738\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  00:59:56 \tepoch:  5 batch:  1000 Training loss:  477.6825442612171 Validation loss  125.39675071835518\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  01:16:36 \tepoch:  6 batch:  1000 Training loss:  469.41102173924446 Validation loss  123.87930300831795\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  01:32:55 \tepoch:  7 batch:  1000 Training loss:  462.2552453428507 Validation loss  125.95111317932606\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  01:49:14 \tepoch:  8 batch:  1000 Training loss:  464.70668333768845 Validation loss  119.49902594089508\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  02:05:33 \tepoch:  9 batch:  1000 Training loss:  460.2712585926056 Validation loss  118.4260675907135\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  02:21:53 \tepoch:  10 batch:  1000 Training loss:  455.60547591745853 Validation loss  116.47648519277573\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  02:38:12 \tepoch:  11 batch:  1000 Training loss:  448.7471304833889 Validation loss  116.66382959485054\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  02:54:31 \tepoch:  12 batch:  1000 Training loss:  450.5872845053673 Validation loss  116.32829427719116\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  03:10:50 \tepoch:  13 batch:  1000 Training loss:  448.10922905802727 Validation loss  116.63535514473915\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  03:27:09 \tepoch:  14 batch:  1000 Training loss:  445.4020319879055 Validation loss  115.40687976777554\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  03:43:28 \tepoch:  15 batch:  1000 Training loss:  443.2434379607439 Validation loss  115.67679271101952\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  03:59:47 \tepoch:  16 batch:  1000 Training loss:  444.67322808504105 Validation loss  115.64215016365051\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  04:16:06 \tepoch:  17 batch:  1000 Training loss:  440.8132074922323 Validation loss  116.94149605929852\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  04:32:25 \tepoch:  18 batch:  1000 Training loss:  441.144286647439 Validation loss  115.64139240980148\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  04:48:44 \tepoch:  19 batch:  1000 Training loss:  438.7166997939348 Validation loss  114.27640990912914\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Time:  05:05:04 \tepoch:  20 batch:  1000 Training loss:  434.87000727653503 Validation loss  117.55512091517448\n",
      "../logging//model-multimodal_e20_lr3e-03_bs 12_gradacc_nodrop.pth\n",
      "Elapsed Training Time:  5:26:58.604051\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# now let's train this thing. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epochs = 20\n",
    "learning_rate = 0.0034\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# make sure the model is running on the GPU if its available\n",
    "testModel.to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "params_to_update = testModel.parameters()\n",
    "optimizer = torch.optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(model500k.parameters(), lr=learning_rate)\n",
    "\n",
    "# start a clock \n",
    "print(\"Training Starting.\")\n",
    "start_time = time.time()\n",
    "\n",
    "# create empty arrays to hold the loss results\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(epochs):\n",
    "    phase = 'train'\n",
    "    # set the model to training mode\n",
    "    testModel.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "# zero the parameter gradients at the very beginning\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # ugh. This is gross. I should have done this step at the beginning for all of the datasets. \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"Here's the size of the inputs: \", inputs.size())\n",
    "        # with torch.set_grad_enabled(phase == 'train'):\n",
    "            # run the training data through the model\n",
    "        outputs = testModel(inputs)\n",
    "\n",
    "        #calculate the loss of the model\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #Gradient accumulation\n",
    "        loss = loss\n",
    "        loss.backward()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # weights update for each gradient accumulation\n",
    "        if ((i + 1) % accum_iter == 0) or (i + 1 == len(train_loader)):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "            if ((i + 1) / accum_iter) % 500 == 0:    # record loss and test validation set every 500 gradient accumulations\n",
    "                testModel.eval()\n",
    "                v_running_loss = 0.0\n",
    "                for v, data in enumerate(val_loader):\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "                    v_outputs = testModel(inputs)\n",
    "                    v_loss = criterion(v_outputs, labels)\n",
    "                    v_running_loss += v_loss.item()\n",
    "\n",
    "                print(\"Time: \", datetime.datetime.now().strftime(\"%H:%M:%S\"), \"\\tepoch: \", epoch+1, \"batch: \", i+1, \"Training loss: \", running_loss, \"Validation loss \", v_running_loss)\n",
    "                validation_losses.append(v_running_loss)\n",
    "                training_losses.append(running_loss)\n",
    "                running_loss = 0.0\n",
    "\n",
    "    model_name= \"model-multimodal_e\" + str(epochs) + f\"_lr{learning_rate:.0e}\" + \"_bs \"+ str(batchsize)\n",
    "    model_filepath=\"../logging//\" + model_name + \"_gradacc_nodrop.pth\"\n",
    "\n",
    "    print(model_filepath)\n",
    "\n",
    "    torch.save(testModel.state_dict(), model_filepath)  \n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "train_time = end_time - start_time\n",
    "print(\"Elapsed Training Time: \", datetime.timedelta(seconds = train_time))\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([9, 14])\n",
      "tensor([[ 0.7777,  0.4924,  0.2549, -2.0368, -0.7276, -0.9207, -1.2739, -0.4648,\n",
      "         -2.7613, -0.7140, -1.5608, -0.6553, -0.4229, -1.6931],\n",
      "        [ 0.9465,  0.8030, -1.0154, -2.7766, -0.5661, -2.6643, -2.9966, -2.0709,\n",
      "         -4.2062, -2.1585, -2.2636, -1.8323, -0.6298, -0.9423],\n",
      "        [ 0.5549,  0.0441,  2.3346, -1.0762, -1.2719,  1.3595,  0.8955,  1.5831,\n",
      "         -1.7152,  1.1046,  0.2225,  1.5098, -1.3559, -2.4570],\n",
      "        [-1.1147, -1.3933, -1.7959, -3.6662, -2.6506, -3.2269, -3.0562, -2.3172,\n",
      "         -4.7559, -2.7542, -2.5362, -1.9790, -1.3874,  0.0630],\n",
      "        [-1.3007, -1.8743, -0.7358, -2.0096, -2.2818, -1.5462, -1.1014, -1.4754,\n",
      "         -3.3545, -1.9182, -1.1387, -0.3059, -1.7324, -0.2215],\n",
      "        [-1.4328, -1.6334, -1.7109, -1.8433, -1.8955, -2.5180, -2.0537, -2.7390,\n",
      "         -3.8142, -2.9839, -2.0316, -1.6495, -1.3078,  0.1542],\n",
      "        [-0.3777, -0.5700, -0.1404, -2.7327, -1.9813, -1.1909, -1.4807, -0.5194,\n",
      "         -2.7538, -0.6618, -1.1917, -0.3580, -0.6934, -1.2024],\n",
      "        [-0.4396, -0.6352, -0.6548, -2.2315, -1.4064, -1.7824, -1.7205, -1.3986,\n",
      "         -3.7080, -1.7381, -1.5952, -0.7975, -1.4432, -0.6199],\n",
      "        [-3.9618, -3.9949, -4.6517, -4.6428, -4.8504, -5.6292, -4.6669, -5.0791,\n",
      "         -6.0142, -5.3605, -3.5616, -3.6619, -1.8815,  2.2248]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.Size([9, 14])\n",
      "tensor([[1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       device='cuda:0')\n",
      "torch.float32\n",
      "torch.float32\n",
      "tensor(0.4597, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "[573.4410420954227, 535.3525197803974, 501.0550397336483, 491.1478876173496, 477.6825442612171, 469.41102173924446, 462.2552453428507, 464.70668333768845, 460.2712585926056, 455.60547591745853, 448.7471304833889, 450.5872845053673, 448.10922905802727, 445.4020319879055, 443.2434379607439, 444.67322808504105, 440.8132074922323, 441.144286647439, 438.7166997939348, 434.87000727653503]\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs))\n",
    "print(outputs.size())\n",
    "#print(outputs[1, :])\n",
    "print(outputs)\n",
    "print(labels.size())\n",
    "#print(labels[1, :])\n",
    "print(labels)\n",
    "print(outputs.dtype)\n",
    "print(labels.dtype)\n",
    "print(loss)\n",
    "\n",
    "print(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1af72950580>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Train and Val Loss\")\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me592cuda",
   "language": "python",
   "name": "me592cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
