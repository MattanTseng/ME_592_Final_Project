{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '../utils/')\n",
    "from dataset import ChestImage64\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import vit_l_16, ViT_L_16_Weights,vit_b_16, ViT_B_16_Weights\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30630, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Front Path</th>\n",
       "      <th>Lateral path</th>\n",
       "      <th>Patient</th>\n",
       "      <th>Study</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>EncodedLabels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frontal\\patient00002_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00002_study1_Lateral.png</td>\n",
       "      <td>patient00002</td>\n",
       "      <td>study1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1,1,0,1,1,1,0,0,1,1,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frontal\\patient00004_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00004_study1_Lateral.png</td>\n",
       "      <td>patient00004</td>\n",
       "      <td>study1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frontal\\patient00005_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00005_study1_Lateral.png</td>\n",
       "      <td>patient00005</td>\n",
       "      <td>study1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,1,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frontal\\patient00009_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00009_study1_Lateral.png</td>\n",
       "      <td>patient00009</td>\n",
       "      <td>study1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frontal\\patient00010_study1_Frontal.png</td>\n",
       "      <td>Lateral\\patient00010_study1_Lateral.png</td>\n",
       "      <td>patient00010</td>\n",
       "      <td>study1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Front Path  \\\n",
       "0  Frontal\\patient00002_study1_Frontal.png   \n",
       "1  Frontal\\patient00004_study1_Frontal.png   \n",
       "2  Frontal\\patient00005_study1_Frontal.png   \n",
       "3  Frontal\\patient00009_study1_Frontal.png   \n",
       "4  Frontal\\patient00010_study1_Frontal.png   \n",
       "\n",
       "                              Lateral path       Patient   Study  \\\n",
       "0  Lateral\\patient00002_study1_Lateral.png  patient00002  study1   \n",
       "1  Lateral\\patient00004_study1_Lateral.png  patient00004  study1   \n",
       "2  Lateral\\patient00005_study1_Lateral.png  patient00005  study1   \n",
       "3  Lateral\\patient00009_study1_Lateral.png  patient00009  study1   \n",
       "4  Lateral\\patient00010_study1_Lateral.png  patient00010  study1   \n",
       "\n",
       "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
       "0                           1             1             1            1      0   \n",
       "1                           0             0             0            0      0   \n",
       "2                           0             0             0            0      0   \n",
       "3                           1             1             0            0      0   \n",
       "4                           0             0             0            0      0   \n",
       "\n",
       "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
       "0              1          1            1             0                 0   \n",
       "1              0          0            0             0                 0   \n",
       "2              0          0            0             0                 0   \n",
       "3              0          0            0             0                 0   \n",
       "4              0          0            0             0                 0   \n",
       "\n",
       "   Pleural Other  Fracture  Support Devices  No Finding  \\\n",
       "0              1         1                0           0   \n",
       "1              0         0                0           1   \n",
       "2              0         0                1           0   \n",
       "3              0         0                0           0   \n",
       "4              0         0                0           1   \n",
       "\n",
       "                 EncodedLabels  \n",
       "0  1,1,1,1,0,1,1,1,0,0,1,1,0,0  \n",
       "1  0,0,0,0,0,0,0,0,0,0,0,0,0,1  \n",
       "2  0,0,0,0,0,0,0,0,0,0,0,0,1,0  \n",
       "3  1,1,0,0,0,0,0,0,0,0,0,0,0,0  \n",
       "4  0,0,0,0,0,0,0,0,0,0,0,0,0,1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"../256pxImages/train_labels_256p_paired.csv\"\n",
    "root_path = '../256pxImages'\n",
    "\n",
    "pairCSV = pd.read_csv(csv_path)\n",
    "pairCSV['EncodedLabels'] = ''\n",
    "print(pairCSV.shape)\n",
    "\n",
    "for i in range(4, pairCSV.shape[1]-1):\n",
    "    pairCSV['EncodedLabels'] = pairCSV['EncodedLabels'].astype(str) + pairCSV.iloc[:, i].astype(str) \n",
    "    if i < pairCSV.shape[1]-2:\n",
    "        pairCSV['EncodedLabels'] = pairCSV['EncodedLabels'].astype(str) + \",\" \n",
    "\n",
    "pairCSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_test:  [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([256, 256])\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "# test image loading\n",
    "front_file = pairCSV.iloc[0, 0]\n",
    "lat_file= pairCSV.iloc[0, 1]\n",
    "label_test = pairCSV['EncodedLabels'].iloc[0]\n",
    "\n",
    "test_path_front = os.path.join(root_path, front_file)\n",
    "test_path_lat = os.path.join(root_path, lat_file)\n",
    "\n",
    "label_test = [int(x) for x in label_test.split(\",\")]\n",
    "\n",
    "print(\"label_test: \", label_test)\n",
    "\n",
    "image_front = io.imread(test_path_front)\n",
    "print(type(image_front))\n",
    "image_front = torch.tensor(image_front)\n",
    "print(image_front.size())\n",
    "\n",
    "image_lat = io.imread(test_path_lat)\n",
    "print(type(image_lat))\n",
    "image_lat = torch.tensor(image_lat)\n",
    "print(image_lat.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the paired dataset\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, label_col, transform = None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # get the filename of the image\n",
    "        file_front = self.df.iloc[index, 0]\n",
    "        file_lat = self.df.iloc[index, 1]\n",
    "        label = self.df[self.label_col].iloc[index]\n",
    "\n",
    "        if type(label) == str:\n",
    "            label = [int(x) for x in label.split(\",\")]\n",
    "\n",
    "        # load the image from disk\n",
    "        front_path = os.path.join(self.root_dir, file_front)\n",
    "        lat_path = os.path.join(self.root_dir, file_lat)\n",
    "\n",
    "        img_front = io.imread(front_path)\n",
    "        img_lat = io.imread(lat_path)\n",
    "\n",
    "        label = torch.tensor(label)\n",
    "        label = label.float()\n",
    "\n",
    "        img_front = torch.tensor(img_front)\n",
    "        img_front = img_front.resize_((224, 224))\n",
    "        img_front = repeat(img_front, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img_front = rearrange(img_front, \"(c h) w -> 1 c h w\", c = 3)\n",
    "        img_front = img_front.float()\n",
    "\n",
    "        img_lat = torch.tensor(img_lat)\n",
    "        img_lat = img_lat.resize_((224, 224))\n",
    "        img_lat = repeat(img_lat, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img_lat = rearrange(img_lat, \"(c h) w -> 1 c h w\", c = 3)\n",
    "        img_lat = img_lat.float()\n",
    "\n",
    "        img_pair=torch.cat((img_front,img_lat),0)\n",
    "\n",
    "\n",
    "        # if self.transform:\n",
    "            # label = self.transform(label)\n",
    "            # img = self.transform(img)\n",
    "\n",
    "        # return the image and its filename\n",
    "        return img_pair, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.PairedDataset'>\n"
     ]
    }
   ],
   "source": [
    "default_transform = ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1.transforms\n",
    "pairDataset = PairedDataset(pairCSV, root_dir=root_path, label_col=\"EncodedLabels\", transform=default_transform)\n",
    "\n",
    "print(type(pairDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length:  21441\n",
      "Validation Length:  3063\n",
      "Test Length:  6126\n",
      "torch.Size([8, 2, 3, 224, 224])\n",
      "torch.float32\n",
      "tensor([[[ 58.,  52.,  48.,  ...,  35.,  35.,  35.],\n",
      "         [ 35.,  35.,  35.,  ...,  38.,  37.,  36.],\n",
      "         [ 36.,  36.,  36.,  ..., 252., 252., 252.],\n",
      "         ...,\n",
      "         [185., 186., 190.,  ..., 228., 229., 229.],\n",
      "         [228., 222., 218.,  ..., 153., 156., 160.],\n",
      "         [162., 169., 178.,  ...,  42.,  45.,  58.]],\n",
      "\n",
      "        [[ 58.,  52.,  48.,  ...,  35.,  35.,  35.],\n",
      "         [ 35.,  35.,  35.,  ...,  38.,  37.,  36.],\n",
      "         [ 36.,  36.,  36.,  ..., 252., 252., 252.],\n",
      "         ...,\n",
      "         [185., 186., 190.,  ..., 228., 229., 229.],\n",
      "         [228., 222., 218.,  ..., 153., 156., 160.],\n",
      "         [162., 169., 178.,  ...,  42.,  45.,  58.]],\n",
      "\n",
      "        [[ 58.,  52.,  48.,  ...,  35.,  35.,  35.],\n",
      "         [ 35.,  35.,  35.,  ...,  38.,  37.,  36.],\n",
      "         [ 36.,  36.,  36.,  ..., 252., 252., 252.],\n",
      "         ...,\n",
      "         [185., 186., 190.,  ..., 228., 229., 229.],\n",
      "         [228., 222., 218.,  ..., 153., 156., 160.],\n",
      "         [162., 169., 178.,  ...,  42.,  45.,  58.]]])\n",
      "torch.Size([8, 14])\n",
      "06:41:16\n"
     ]
    }
   ],
   "source": [
    "# split into test train validate\n",
    "train_size = int(0.7 * len(pairDataset))\n",
    "val_size = int(0.1 * len(pairDataset))\n",
    "test_size = int(0.2 * len(pairDataset))\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(pairDataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(\"Train Length: \", len(train_dataset))\n",
    "print(\"Validation Length: \", len(val_dataset))\n",
    "print(\"Test Length: \", len(test_dataset))\n",
    "\n",
    "batchsize = 8\n",
    "\n",
    "# make three different dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "features, labels = next(iter(train_loader))\n",
    "print(features.size())\n",
    "print(features.dtype)\n",
    "\n",
    "print(features[1, 1, :, :])\n",
    "\n",
    "print(labels.size())\n",
    "print(datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "front_features=features[:, 0, :, :, :]\n",
    "print(front_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "lat_features=features[:, 1, :, :, :]\n",
    "print(lat_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllAttentionVIT(nn.Module):\n",
    "    def __init__(self,weights_frontal,weights_lateral,code_size,num_classes,bottleneck=1024,drop_rate=0.1):\n",
    "        \n",
    "        super(AllAttentionVIT,self).__init__()\n",
    "\n",
    "        #Initialize the model\n",
    "        self.transformer_enc_frontal = vit_b_16(weights = weights_frontal)\n",
    "        self.transformer_enc_frontal.heads = nn.Sequential(nn.Linear(768, code_size))\n",
    "        self.transformer_enc_lateral = vit_b_16(weights = weights_lateral)\n",
    "        self.transformer_enc_lateral.heads = nn.Sequential(nn.Linear(768, code_size))\n",
    "\n",
    "        self.ff1 = nn.Linear(code_size*2,bottleneck)\n",
    "        self.dropout = nn.Dropout(p = drop_rate)\n",
    "        self.ff2 = nn.Linear(bottleneck,num_classes)\n",
    "\n",
    "    #Expect two images with the same class.\n",
    "    def forward(self,x):\n",
    "\n",
    "        #Split up paired images\n",
    "        x_f=x[:, 0, :, :, :]\n",
    "        x_l=x[:, 1, :, :, :]\n",
    "        \n",
    "        #Encode front and lateral embeddings\n",
    "        x_f = self.transformer_enc_frontal(x_f)\n",
    "        x_l = self.transformer_enc_frontal(x_l)\n",
    "\n",
    "        #Concat embeddings. May want to experiment with convolutions.\n",
    "        x = torch.cat((x_f,x_l),1)\n",
    "\n",
    "        #Map concatenated embeddings onto the classes\n",
    "        x = self.ff1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.ff2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174366734\n"
     ]
    }
   ],
   "source": [
    "testWeights=ViT_B_16_Weights.DEFAULT\n",
    "testModel = AllAttentionVIT(testWeights,testWeights,768,14)\n",
    "\n",
    "# simple function to determine how many TRAINABILE parameters are in the model\n",
    "def count_parameters(testModel):\n",
    "    return sum(p.numel() for p in testModel.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(testModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t transformer_enc_frontal.class_token\n",
      "\t transformer_enc_frontal.conv_proj.weight\n",
      "\t transformer_enc_frontal.conv_proj.bias\n",
      "\t transformer_enc_frontal.encoder.pos_embedding\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_6.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_7.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_8.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_9.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_1.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_1.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_2.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.ln_2.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "\t transformer_enc_frontal.encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "\t transformer_enc_frontal.encoder.ln.weight\n",
      "\t transformer_enc_frontal.encoder.ln.bias\n",
      "\t transformer_enc_frontal.heads.0.weight\n",
      "\t transformer_enc_frontal.heads.0.bias\n",
      "\t transformer_enc_lateral.class_token\n",
      "\t transformer_enc_lateral.conv_proj.weight\n",
      "\t transformer_enc_lateral.conv_proj.bias\n",
      "\t transformer_enc_lateral.encoder.pos_embedding\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_6.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_7.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_8.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_9.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_1.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_1.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_2.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.ln_2.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "\t transformer_enc_lateral.encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "\t transformer_enc_lateral.encoder.ln.weight\n",
      "\t transformer_enc_lateral.encoder.ln.bias\n",
      "\t transformer_enc_lateral.heads.0.weight\n",
      "\t transformer_enc_lateral.heads.0.bias\n",
      "\t ff1.weight\n",
      "\t ff1.bias\n",
      "\t ff2.weight\n",
      "\t ff2.bias\n"
     ]
    }
   ],
   "source": [
    "# print out a list of the parameters we are training\n",
    "for name,param in testModel.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch accumulation parameter\n",
    "target_accumulation=32\n",
    "accum_iter = target_accumulation/batchsize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print(accum_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Starting.\n",
      "Time:  07:48:34 \tepoch:  1 batch:  100 Training loss:  15.887869402766228 Validation loss  222.91519504785538\n",
      "Time:  07:51:00 \tepoch:  1 batch:  200 Training loss:  14.720197632908821 Validation loss  218.76152348518372\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m# zero the parameter gradients at the very beginning\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     36\u001b[0m         inputs, labels \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     38\u001b[0m         \u001b[39m# ugh. This is gross. I should have done this step at the beginning for all of the datasets. \u001b[39;00m\n\u001b[0;32m     39\u001b[0m         \u001b[39m# zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "Cell \u001b[1;32mIn[26], line 29\u001b[0m, in \u001b[0;36mPairedDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     26\u001b[0m front_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir, file_front)\n\u001b[0;32m     27\u001b[0m lat_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir, file_lat)\n\u001b[1;32m---> 29\u001b[0m img_front \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mimread(front_path)\n\u001b[0;32m     30\u001b[0m img_lat \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mimread(lat_path)\n\u001b[0;32m     32\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(label)\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\skimage\\io\\_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m     50\u001b[0m         plugin \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtifffile\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m file_or_url_context(fname) \u001b[39mas\u001b[39;00m fname:\n\u001b[1;32m---> 53\u001b[0m     img \u001b[39m=\u001b[39m call_plugin(\u001b[39m'\u001b[39m\u001b[39mimread\u001b[39m\u001b[39m'\u001b[39m, fname, plugin\u001b[39m=\u001b[39mplugin, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mplugin_args)\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(img, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\skimage\\io\\manage_plugins.py:207\u001b[0m, in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not find the plugin \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    205\u001b[0m                            (plugin, kind))\n\u001b[1;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py:15\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m@wraps\u001b[39m(imageio_imread)\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(imageio_imread(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\v2.py:200\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m imopen_args \u001b[39m=\u001b[39m decypher_format_arg(\u001b[39mformat\u001b[39m)\n\u001b[0;32m    198\u001b[0m imopen_args[\u001b[39m\"\u001b[39m\u001b[39mlegacy_mode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[39mwith\u001b[39;00m imopen(uri, \u001b[39m\"\u001b[39m\u001b[39mri\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mimopen_args) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m    201\u001b[0m     \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mread(index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\core\\imopen.py:118\u001b[0m, in \u001b[0;36mimopen\u001b[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     request\u001b[39m.\u001b[39mformat_hint \u001b[39m=\u001b[39m format_hint\n\u001b[0;32m    117\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     request \u001b[39m=\u001b[39m Request(uri, io_mode, format_hint\u001b[39m=\u001b[39;49mformat_hint, extension\u001b[39m=\u001b[39;49mextension)\n\u001b[0;32m    120\u001b[0m source \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<bytes>\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(uri, \u001b[39mbytes\u001b[39m) \u001b[39melse\u001b[39;00m uri\n\u001b[0;32m    122\u001b[0m \u001b[39m# fast-path based on plugin\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# (except in legacy mode)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\core\\request.py:248\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[1;34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid Request.Mode: \u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    247\u001b[0m \u001b[39m# Parse what was given\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_uri(uri)\n\u001b[0;32m    250\u001b[0m \u001b[39m# Set extension\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m extension \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\site-packages\\imageio\\core\\request.py:381\u001b[0m, in \u001b[0;36mRequest._parse_uri\u001b[1;34m(self, uri)\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filename_zip:\n\u001b[0;32m    380\u001b[0m         fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filename_zip[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 381\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mexists(fn)) \u001b[39mand\u001b[39;00m (fn \u001b[39min\u001b[39;00m EXAMPLE_IMAGES):\n\u001b[0;32m    382\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[0;32m    383\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mNo such file: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m. This file looks like one of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe standard images, but from imageio 2.1, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mstandard images have to be specified using \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m             \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimageio:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (fn, fn)\n\u001b[0;32m    387\u001b[0m         )\n\u001b[0;32m    389\u001b[0m \u001b[39m# Make filename absolute\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\machi\\.conda\\envs\\me592cuda\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     20\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now let's train this thing. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epochs = 1\n",
    "learning_rate = 0.003\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# make sure the model is running on the GPU if its available\n",
    "testModel.to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "params_to_update = testModel.parameters()\n",
    "optimizer = torch.optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(model500k.parameters(), lr=learning_rate)\n",
    "\n",
    "# start a clock \n",
    "print(\"Training Starting.\")\n",
    "start_time = time.time()\n",
    "\n",
    "# create empty arrays to hold the loss results\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(epochs):\n",
    "    phase = 'train'\n",
    "    # set the model to training mode\n",
    "    testModel.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "# zero the parameter gradients at the very beginning\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # ugh. This is gross. I should have done this step at the beginning for all of the datasets. \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"Here's the size of the inputs: \", inputs.size())\n",
    "        # with torch.set_grad_enabled(phase == 'train'):\n",
    "            # run the training data through the model\n",
    "        outputs = testModel(inputs)\n",
    "\n",
    "        #calculate the loss of the model\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #Gradient accumulation\n",
    "        loss = loss / accum_iter\n",
    "        loss.backward()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # weights update for each gradient accumulation\n",
    "        if ((i + 1) % accum_iter == 0) or (i + 1 == len(train_loader)):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            if ((i + 1) / accum_iter) % 50 == 0:    # record loss and test validation set every 50 gradient accumulations\n",
    "                testModel.eval()\n",
    "                v_running_loss = 0.0\n",
    "                for v, data in enumerate(val_loader):\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                    v_outputs = testModel(inputs)\n",
    "                    v_loss = criterion(v_outputs, labels)\n",
    "                    v_running_loss += v_loss.item()\n",
    "\n",
    "                print(\"Time: \", datetime.datetime.now().strftime(\"%H:%M:%S\"), \"\\tepoch: \", epoch+1, \"batch: \", i+1, \"Training loss: \", running_loss, \"Validation loss \", v_running_loss)\n",
    "                validation_losses.append(v_running_loss)\n",
    "                training_losses.append(running_loss)\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "train_time = end_time - start_time\n",
    "print(\"Elapsed Training Time: \", datetime.timedelta(seconds = train_time))\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 14])\n",
      "tensor([[-2.2176, -2.6391, -2.5167, -2.8809, -3.2126, -2.8804, -2.2874, -2.7287,\n",
      "         -3.9277, -2.9954, -1.6956, -1.4155, -1.4414,  0.8752]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 14])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       device='cuda:0')\n",
      "torch.float32\n",
      "torch.float32\n",
      "tensor(0.1124, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "[59.11822012066841, 58.40444514155388, 58.676900178194046, 57.259695529937744, 55.94352215528488, 55.36232903599739, 54.021388083696365, 56.52964359521866, 56.32263672351837, 55.36959132552147, 52.960826486349106, 53.664486050605774, 52.42195749282837, 50.19291827082634, 50.356932282447815, 50.969821989536285, 51.904568552970886, 52.196694761514664, 50.74101144075394, 51.431266874074936, 51.28189331293106, 50.09650614857674, 50.017876625061035, 49.72847095131874, 51.051646798849106, 50.411791652441025]\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs))\n",
    "print(outputs.size())\n",
    "#print(outputs[1, :])\n",
    "print(outputs)\n",
    "print(labels.size())\n",
    "#print(labels[1, :])\n",
    "print(labels)\n",
    "print(outputs.dtype)\n",
    "print(labels.dtype)\n",
    "print(loss)\n",
    "\n",
    "print(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d1d7067dc0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Train and Val Loss\")\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me592cuda",
   "language": "python",
   "name": "me592cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
