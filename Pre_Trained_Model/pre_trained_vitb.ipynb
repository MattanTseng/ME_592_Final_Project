{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '../utils/')\n",
    "from dataset import ChestImage64\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import vit_l_16, ViT_L_16_Weights,vit_b_16, ViT_B_16_Weights\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_path = \"../64pxImages/train_labels_64p.csv\"\n",
    "# root_path = '../64pxImages'\n",
    "\n",
    "csv_path = \"../256pxImages/train_labels_256p.csv\"\n",
    "root_path = '../256pxImages'\n",
    "\n",
    "\n",
    "default_transform = ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1.transforms\n",
    "\n",
    "\n",
    "data_transform = Compose([\n",
    "    Resize((64, 64)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61266, 19)\n",
      "label_test:  [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "myCSV = pd.read_csv(csv_path)\n",
    "myCSV['EncodedLabels'] = ''\n",
    "print(myCSV.shape)\n",
    "\n",
    "\n",
    "# for i in range(4, myCSV.shape[1]-1):\n",
    "#     myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + myCSV.iloc[:, i].astype(str) \n",
    "#     if i < myCSV.shape[1]-2:\n",
    "#         myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + \",\" \n",
    "\n",
    "for i in range(4, myCSV.shape[1]-1):\n",
    "    myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + myCSV.iloc[:, i].astype(str) \n",
    "    if i < myCSV.shape[1]-2:\n",
    "        myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + \",\" \n",
    "\n",
    "\n",
    "\n",
    "# myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + \"]\"\n",
    "\n",
    "\n",
    "# We can use the encodedlabels column as our labels for our data\n",
    "\n",
    "# since we are not useing cross attention, pull out only the frontal images. \n",
    "frontalCSV = myCSV[myCSV['Frontal/Lateral'].str.contains(\"Frontal\")]\n",
    "frontalCSV.head()\n",
    "\n",
    "filename = frontalCSV.iloc[1, 0]\n",
    "label_test = frontalCSV['EncodedLabels'].iloc[0]\n",
    "\n",
    "test_path = os.path.join(root_path, filename)\n",
    "\n",
    "\n",
    "label_test = [int(x) for x in label_test.split(\",\")]\n",
    "\n",
    "print(\"label_test: \", label_test)\n",
    "\n",
    "image = io.imread(test_path)\n",
    "print(type(image))\n",
    "image = torch.tensor(image)\n",
    "print(image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>256path</th>\n",
       "      <th>Patient</th>\n",
       "      <th>Study</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>EncodedLabels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frontal\\patient00002_study1_Frontal.png</td>\n",
       "      <td>patient00002</td>\n",
       "      <td>study1</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1,1,0,1,1,1,0,0,1,1,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lateral\\patient00002_study1_Lateral.png</td>\n",
       "      <td>patient00002</td>\n",
       "      <td>study1</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1,1,0,1,1,1,0,0,1,1,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frontal\\patient00004_study1_Frontal.png</td>\n",
       "      <td>patient00004</td>\n",
       "      <td>study1</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lateral\\patient00004_study1_Lateral.png</td>\n",
       "      <td>patient00004</td>\n",
       "      <td>study1</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frontal\\patient00005_study1_Frontal.png</td>\n",
       "      <td>patient00005</td>\n",
       "      <td>study1</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,1,0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   256path       Patient   Study  \\\n",
       "0  Frontal\\patient00002_study1_Frontal.png  patient00002  study1   \n",
       "1  Lateral\\patient00002_study1_Lateral.png  patient00002  study1   \n",
       "2  Frontal\\patient00004_study1_Frontal.png  patient00004  study1   \n",
       "3  Lateral\\patient00004_study1_Lateral.png  patient00004  study1   \n",
       "4  Frontal\\patient00005_study1_Frontal.png  patient00005  study1   \n",
       "\n",
       "  Frontal/Lateral  Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  \\\n",
       "0         Frontal                           1             1             1   \n",
       "1         Lateral                           1             1             1   \n",
       "2         Frontal                           0             0             0   \n",
       "3         Lateral                           0             0             0   \n",
       "4         Frontal                           0             0             0   \n",
       "\n",
       "   Lung Lesion  Edema  Consolidation  Pneumonia  Atelectasis  Pneumothorax  \\\n",
       "0            1      0              1          1            1             0   \n",
       "1            1      0              1          1            1             0   \n",
       "2            0      0              0          0            0             0   \n",
       "3            0      0              0          0            0             0   \n",
       "4            0      0              0          0            0             0   \n",
       "\n",
       "   Pleural Effusion  Pleural Other  Fracture  Support Devices  No Finding  \\\n",
       "0                 0              1         1                0           0   \n",
       "1                 0              1         1                0           0   \n",
       "2                 0              0         0                0           1   \n",
       "3                 0              0         0                0           1   \n",
       "4                 0              0         0                1           0   \n",
       "\n",
       "                 EncodedLabels  \n",
       "0  1,1,1,1,0,1,1,1,0,0,1,1,0,0  \n",
       "1  1,1,1,1,0,1,1,1,0,0,1,1,0,0  \n",
       "2  0,0,0,0,0,0,0,0,0,0,0,0,0,1  \n",
       "3  0,0,0,0,0,0,0,0,0,0,0,0,0,1  \n",
       "4  0,0,0,0,0,0,0,0,0,0,0,0,1,0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CustomDataset'>\n",
      "Train Length:  21441\n",
      "Validation Length:  3063\n",
      "Test Length:  6126\n",
      "torch.Size([16, 3, 224, 224])\n",
      "torch.float32\n",
      "tensor([[213., 213., 212.,  ...,  53.,  53.,  53.],\n",
      "        [ 53.,  52.,  52.,  ...,  32.,  32.,  32.],\n",
      "        [ 31.,  31.,  32.,  ...,  30.,  30.,  30.],\n",
      "        ...,\n",
      "        [217., 210., 205.,  ..., 124., 134., 158.],\n",
      "        [159., 151., 160.,  ..., 180., 183., 187.],\n",
      "        [194., 195., 199.,  ...,  24.,  28.,  32.]])\n",
      "torch.Size([16, 14])\n",
      "07:53:06\n"
     ]
    }
   ],
   "source": [
    "# load up the dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, label_col, transform = None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # get the filename of the image\n",
    "        filename = self.df.iloc[index, 0]\n",
    "        label = self.df[self.label_col].iloc[index]\n",
    "\n",
    "        if type(label) == str:\n",
    "            label = [int(x) for x in label.split(\",\")]\n",
    "\n",
    "        # load the image from disk\n",
    "        path = os.path.join(self.root_dir, filename)\n",
    "        img = io.imread(path)\n",
    "\n",
    "\n",
    "\n",
    "        label = torch.tensor(label)\n",
    "        label = label.float()\n",
    "        img = torch.tensor(img)\n",
    "        img = img.resize_((224, 224))\n",
    "        img = repeat(img, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img = rearrange(img, \"(c h) w -> c h w\", c = 3)\n",
    "        img = img.float()\n",
    "\n",
    "\n",
    "        # if self.transform:\n",
    "            # label = self.transform(label)\n",
    "            # img = self.transform(img)\n",
    "\n",
    "        # return the image and its filename\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "dataset = CustomDataset(frontalCSV, root_dir=root_path, label_col=\"EncodedLabels\", transform=default_transform)\n",
    "\n",
    "print(type(dataset))\n",
    "\n",
    "# split into test train validate\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = int(0.2 * len(dataset))\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(\"Train Length: \", len(train_dataset))\n",
    "print(\"Validation Length: \", len(val_dataset))\n",
    "print(\"Test Length: \", len(test_dataset))\n",
    "\n",
    "batchsize = 16\n",
    "\n",
    "# make three different dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "features, labels = next(iter(train_loader))\n",
    "print(features.size())\n",
    "print(features.dtype)\n",
    "\n",
    "print(features[1, 1, :, :])\n",
    "\n",
    "print(labels.size())\n",
    "print(datetime.datetime.now().strftime(\"%H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about the pretrained models is coming from this link: \n",
    "#https://pytorch.org/vision/master/models.html\n",
    "\n",
    "\n",
    "# just use the default weights. These should yeild the best results\n",
    "#weights = ViT_L_16_Weights.DEFAULT\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "num_classes = 14\n",
    "feature_extraction = False\n",
    "model = vit_b_16(weights = weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "learning_rate = 0.003\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "if feature_extraction: \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # change the last layer to have the correct number of classes\n",
    "    #model.heads = nn.Sequential(nn.Linear(1024, num_classes))\n",
    "    model.heads = nn.Sequential(nn.Linear(768, num_classes))\n",
    "    model.heads.requires_grad_ = True\n",
    "else:\n",
    "        #model.heads = nn.Sequential(nn.Linear(1024, num_classes))\n",
    "        model.heads = nn.Sequential(nn.Linear(768, num_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t class_token\n",
      "\t conv_proj.weight\n",
      "\t conv_proj.bias\n",
      "\t encoder.pos_embedding\n",
      "\t encoder.layers.encoder_layer_0.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_0.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_0.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_0.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_1.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_1.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_1.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_1.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_2.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_2.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_2.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_2.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_3.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_3.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_3.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_3.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_4.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_4.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_4.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_4.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_5.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_5.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_5.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_5.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_6.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_6.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_6.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_6.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_6.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_6.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_6.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_6.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_7.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_7.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_7.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_7.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_7.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_7.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_7.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_7.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_8.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_8.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_8.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_8.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_8.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_8.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_8.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_8.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_9.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_9.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_9.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_9.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_9.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_9.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_9.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_9.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_10.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_10.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_10.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_10.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_11.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_11.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_11.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_11.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "\t encoder.ln.weight\n",
      "\t encoder.ln.bias\n",
      "\t heads.0.weight\n",
      "\t heads.0.bias\n"
     ]
    }
   ],
   "source": [
    "params_to_update =model.parameters()\n",
    "\n",
    "# print out a list of the parameters we are training\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (self_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (mlp): MLPBlock(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (4): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers.encoder_layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# batch accumulation parameter\n",
    "target_accumulation=32\n",
    "accum_iter = target_accumulation/batchsize  \n",
    "print(accum_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t class_token\n",
      "\t conv_proj.weight\n",
      "\t conv_proj.bias\n",
      "\t encoder.pos_embedding\n",
      "\t encoder.layers.encoder_layer_0.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_0.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_0.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_0.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_1.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_1.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_1.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_1.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_2.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_2.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_2.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_2.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_3.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_3.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_3.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_3.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_4.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_4.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_4.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_4.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_5.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_5.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_5.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_5.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_6.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_6.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_6.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_6.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_6.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_6.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_6.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_6.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_7.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_7.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_7.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_7.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_7.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_7.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_7.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_7.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_8.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_8.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_8.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_8.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_8.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_8.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_8.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_8.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_9.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_9.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_9.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_9.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_9.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_9.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_9.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_9.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_10.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_10.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_10.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_10.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_11.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_11.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_11.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_11.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "\t encoder.ln.weight\n",
      "\t encoder.ln.bias\n",
      "\t heads.0.weight\n",
      "\t heads.0.bias\n",
      "Training Starting.\n",
      "Time:  07:54:40 \tepoch:  1 batch:  100 Training loss:  28.735220670700073 Validation loss  111.61401653289795\n",
      "Time:  07:55:52 \tepoch:  1 batch:  200 Training loss:  27.884246051311493 Validation loss  103.67186504602432\n",
      "Time:  07:57:05 \tepoch:  1 batch:  300 Training loss:  27.36662322282791 Validation loss  104.82783907651901\n",
      "Time:  07:58:13 \tepoch:  1 batch:  400 Training loss:  26.481870725750923 Validation loss  100.47869598865509\n",
      "Time:  07:59:20 \tepoch:  1 batch:  500 Training loss:  26.65985071659088 Validation loss  101.15931755304337\n",
      "Time:  08:00:29 \tepoch:  1 batch:  600 Training loss:  25.885206922888756 Validation loss  97.36049363017082\n",
      "Time:  08:01:39 \tepoch:  1 batch:  700 Training loss:  25.313614949584007 Validation loss  99.07719302177429\n",
      "Time:  08:02:49 \tepoch:  1 batch:  800 Training loss:  25.706057280302048 Validation loss  96.2539211511612\n",
      "Time:  08:04:01 \tepoch:  1 batch:  900 Training loss:  25.368346750736237 Validation loss  98.37172564864159\n",
      "Time:  08:05:06 \tepoch:  1 batch:  1000 Training loss:  25.53947375714779 Validation loss  96.88445493578911\n",
      "Time:  08:06:17 \tepoch:  1 batch:  1100 Training loss:  25.680681452155113 Validation loss  97.46784296631813\n",
      "Time:  08:07:27 \tepoch:  1 batch:  1200 Training loss:  25.731747463345528 Validation loss  97.17176476120949\n",
      "Time:  08:08:38 \tepoch:  1 batch:  1300 Training loss:  24.923182263970375 Validation loss  94.86438125371933\n",
      "Elapsed Training Time:  0:15:33.274855\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# now let's train this thing. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# make sure the model is running on the GPU if its available\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "params_to_update =model.parameters()\n",
    "\n",
    "# print out a list of the parameters we are training\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(model500k.parameters(), lr=learning_rate)\n",
    "\n",
    "# start a clock \n",
    "print(\"Training Starting.\")\n",
    "start_time = time.time()\n",
    "\n",
    "# create empty arrays to hold the loss results\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(epochs):\n",
    "    phase = 'train'\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # zero the parameter gradients at the very beginning\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        # with torch.set_grad_enabled(phase == 'train'):\n",
    "        # run the training data through the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        #calculate the loss of the model\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #Gradient accumulation\n",
    "        loss = loss / accum_iter\n",
    "        loss.backward()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # weights update\n",
    "        if ((i + 1) % accum_iter == 0) or (i + 1 == len(train_loader)):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            if ((i + 1) / accum_iter) % 50 == 0:    # record loss and test validation set every 50 gradient accumulations\n",
    "                model.eval()\n",
    "                v_running_loss = 0.0\n",
    "                for v, data in enumerate(val_loader):\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                    v_outputs = model(inputs)\n",
    "                    v_loss = criterion(v_outputs, labels)\n",
    "                    v_running_loss += v_loss.item()\n",
    "\n",
    "                print(\"Time: \", datetime.datetime.now().strftime(\"%H:%M:%S\"), \"\\tepoch: \", epoch+1, \"batch: \", i+1, \"Training loss: \", running_loss, \"Validation loss \", v_running_loss)\n",
    "                validation_losses.append(v_running_loss)\n",
    "                training_losses.append(running_loss)\n",
    "                running_loss = 0.0\n",
    "        \n",
    "\n",
    "        # once the validation has been completed, update the model\n",
    "       \n",
    "        #optimizer.step()\n",
    "        # set model back to training mode\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "train_time = end_time - start_time\n",
    "print(\"Elapsed Training Time: \", datetime.timedelta(seconds = train_time))\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 14])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(outputs))\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39msize())\n\u001b[1;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(outputs[\u001b[39m1\u001b[39;49m, :])\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(labels\u001b[39m.\u001b[39msize())\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(labels[\u001b[39m1\u001b[39m, :])\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "print(type(outputs))\n",
    "print(outputs.size())\n",
    "print(outputs[1, :])\n",
    "print(labels.size())\n",
    "print(labels[1, :])\n",
    "print(outputs.dtype)\n",
    "print(labels.dtype)\n",
    "print(loss)\n",
    "\n",
    "print(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x283ad659720>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Train and Val Loss\")\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me592cuda",
   "language": "python",
   "name": "me592cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
