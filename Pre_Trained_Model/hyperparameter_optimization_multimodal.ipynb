{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba2c58-ae24-4bc4-bba2-104d8feb885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '../utils/')\n",
    "from dataset import ChestImage64\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import vit_l_16, ViT_L_16_Weights,vit_b_16, ViT_B_16_Weights\n",
    "from torch.utils.data import random_split, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1595199-4e45-4c84-b410-9b90afb5c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"../Data/256pxImages/train_labels_256p_paired.csv\"\n",
    "root_path = '../Data/256pxImages'\n",
    "\n",
    "pairCSV = pd.read_csv(csv_path)\n",
    "pairCSV['EncodedLabels'] = ''\n",
    "print(pairCSV.shape)\n",
    "\n",
    "for i in range(4, pairCSV.shape[1]-1):\n",
    "    pairCSV['EncodedLabels'] = pairCSV['EncodedLabels'].astype(str) + pairCSV.iloc[:, i].astype(str) \n",
    "    if i < pairCSV.shape[1]-2:\n",
    "        pairCSV['EncodedLabels'] = pairCSV['EncodedLabels'].astype(str) + \",\" \n",
    "\n",
    "pairCSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25897df6-ece8-4a98-b0c0-190040653832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test image loading\n",
    "front_file = pairCSV.iloc[0, 0]\n",
    "lat_file= pairCSV.iloc[0, 1]\n",
    "label_test = pairCSV['EncodedLabels'].iloc[0]\n",
    "\n",
    "test_path_front = os.path.join(root_path, front_file)\n",
    "test_path_lat = os.path.join(root_path, lat_file)\n",
    "\n",
    "label_test = [int(x) for x in label_test.split(\",\")]\n",
    "\n",
    "print(\"label_test: \", label_test)\n",
    "\n",
    "image_front = io.imread(test_path_front)\n",
    "print(type(image_front))\n",
    "image_front = torch.tensor(image_front)\n",
    "print(image_front.size())\n",
    "\n",
    "image_lat = io.imread(test_path_lat)\n",
    "print(type(image_lat))\n",
    "image_lat = torch.tensor(image_lat)\n",
    "print(image_lat.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc79185-1584-4408-8107-4fdc677c14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the paired dataset\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, label_col, transform = None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # get the filename of the image\n",
    "        file_front = self.df.iloc[index, 0]\n",
    "        file_lat = self.df.iloc[index, 1]\n",
    "        label = self.df[self.label_col].iloc[index]\n",
    "\n",
    "        if type(label) == str:\n",
    "            label = [int(x) for x in label.split(\",\")]\n",
    "\n",
    "        # load the image from disk\n",
    "        front_path = os.path.join(self.root_dir, file_front)\n",
    "        lat_path = os.path.join(self.root_dir, file_lat)\n",
    "\n",
    "        img_front = io.imread(front_path)\n",
    "        img_lat = io.imread(lat_path)\n",
    "\n",
    "        label = torch.tensor(label)\n",
    "        label = label.float()\n",
    "\n",
    "        img_front = torch.tensor(img_front)\n",
    "        img_front = img_front.resize_((224, 224))\n",
    "        img_front = repeat(img_front, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img_front = rearrange(img_front, \"(c h) w -> 1 c h w\", c = 3)\n",
    "        img_front = img_front.float()\n",
    "\n",
    "        img_lat = torch.tensor(img_lat)\n",
    "        img_lat = img_lat.resize_((224, 224))\n",
    "        img_lat = repeat(img_lat, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img_lat = rearrange(img_lat, \"(c h) w -> 1 c h w\", c = 3)\n",
    "        img_lat = img_lat.float()\n",
    "\n",
    "        img_pair=torch.cat((img_front,img_lat),0)\n",
    "\n",
    "\n",
    "        # if self.transform:\n",
    "            # label = self.transform(label)\n",
    "            # img = self.transform(img)\n",
    "\n",
    "        # return the image and its filename\n",
    "        return img_pair, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe39b6-02bd-4a38-8a30-ea3881fa8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_transform = ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1.transforms\n",
    "#pairDataset = PairedDataset(pairCSV, root_dir=root_path, label_col=\"EncodedLabels\", transform=default_transform)\n",
    "\n",
    "#print(type(pairDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14088383-f278-4b35-a460-5432a8f9bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test train validate\n",
    "#train_size = int(0.7 * len(pairDataset))\n",
    "#val_size = int(0.1 * len(pairDataset))\n",
    "#test_size = int(0.2 * len(pairDataset))\n",
    "\n",
    "\n",
    "#train_dataset, val_dataset, test_dataset = random_split(pairDataset, [train_size, val_size, test_size])\n",
    "\n",
    "#print(\"Train Length: \", len(train_dataset))\n",
    "#print(\"Validation Length: \", len(val_dataset))\n",
    "#print(\"Test Length: \", len(test_dataset))\n",
    "\n",
    "#batchsize = 8\n",
    "\n",
    "# make three different dataloaders\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batchsize, shuffle=False)\n",
    "#test_loader = DataLoader(test_dataset,batch_size=batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "#features, labels = next(iter(train_loader))\n",
    "#print(features.size())\n",
    "#print(features.dtype)\n",
    "\n",
    "#print(features[1, 1, :, :])\n",
    "\n",
    "#print(labels.size())\n",
    "#print(datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be191acd-1326-4417-8c4a-ec768ab96da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "front_features=features[:, 0, :, :, :]\n",
    "print(front_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1b633-6579-483d-996f-697a0c836f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_features=features[:, 1, :, :, :]\n",
    "print(lat_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8875744-33f1-4768-a519-96838f1d9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllAttentionVIT(nn.Module):\n",
    "    def __init__(self,weights_frontal,weights_lateral,code_size,num_classes,bottleneck=1024,drop_rate=0.1):\n",
    "        \n",
    "        super(AllAttentionVIT,self).__init__()\n",
    "\n",
    "        #Initialize the model\n",
    "        self.transformer_enc_frontal = vit_b_16(weights = weights_frontal)\n",
    "        self.transformer_enc_frontal.heads = nn.Sequential(nn.Linear(768, code_size))\n",
    "        self.transformer_enc_lateral = vit_b_16(weights = weights_lateral)\n",
    "        self.transformer_enc_lateral.heads = nn.Sequential(nn.Linear(768, code_size))\n",
    "\n",
    "        self.dropout = nn.Dropout(p = drop_rate)\n",
    "        self.ff = nn.Linear(code_size*2,num_classes)\n",
    "\n",
    "    #Expect two images with the same class.\n",
    "    def forward(self,x):\n",
    "\n",
    "        #Split up paired images\n",
    "        x_f=x[:, 0, :, :, :]\n",
    "        x_l=x[:, 1, :, :, :]\n",
    "        \n",
    "        #Encode front and lateral embeddings\n",
    "        x_f = self.transformer_enc_frontal(x_f)\n",
    "        x_l = self.transformer_enc_frontal(x_l)\n",
    "\n",
    "        #Concat embeddings. May want to experiment with convolutions.\n",
    "        x = torch.cat((x_f,x_l),1)\n",
    "\n",
    "        #Map concatenated embeddings onto the classes\n",
    "        x = self.dropout(x)\n",
    "        x = self.ff(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34222f22-955c-45e7-9203-60e3a7214aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testWeights=ViT_B_16_Weights.DEFAULT\n",
    "#testModel = AllAttentionVIT(testWeights,testWeights,768,14)\n",
    "\n",
    "# simple function to determine how many TRAINABILE parameters are in the model\n",
    "def count_parameters(testModel):\n",
    "    return sum(p.numel() for p in testModel.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(testModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2707e-c1bb-470f-bc6e-f9d1cbcbdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out a list of the parameters we are training\n",
    "for name,param in testModel.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c4e9e-9bac-4e25-9872-e6ac4654c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch accumulation parameter\n",
    "target_accumulation=32\n",
    "accum_iter = target_accumulation/batchsize  \n",
    "print(accum_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7c58e-4855-4bc2-955d-53297b07e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6cc6fb-6b35-4642-a779-5a731dc6a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multimodal_model(train_dataset, validation_dataset,config, benchmark=0.33):\n",
    "    # now let's train this thing. \n",
    "    \n",
    "    batchsize = 8\n",
    "    epochs = 1\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # split into test train validate\n",
    "    train_size = int(0.7 * len(pairDataset))\n",
    "    val_size = int(0.1 * len(pairDataset))\n",
    "    test_size = int(0.2 * len(pairDataset))\n",
    "\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(pairDataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    default_transform = ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1.transforms\n",
    "    pairDataset = PairedDataset(pairCSV, root_dir=root_path, label_col=\"EncodedLabels\", transform=default_transform)\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batchsize, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset,batch_size=batchsize, shuffle=True)\n",
    "    \n",
    "    features, labels = next(iter(train_loader))\n",
    "    \n",
    "    testWeights=ViT_B_16_Weights.DEFAULT\n",
    "    testModel = AllAttentionVIT(testWeights,testWeights,768,14)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # make sure the model is running on the GPU if its available\n",
    "    testModel.to(device)\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    params_to_update = testModel.parameters()\n",
    "    optimizer = torch.optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "    # optimizer = optim.Adam(model500k.parameters(), lr=learning_rate)\n",
    "\n",
    "    # start a clock \n",
    "    print(\"Training Starting.\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # create empty arrays to hold the loss results\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        phase = 'train'\n",
    "        # set the model to training mode\n",
    "        testModel.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "    # zero the parameter gradients at the very beginning\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # ugh. This is gross. I should have done this step at the beginning for all of the datasets. \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"Here's the size of the inputs: \", inputs.size())\n",
    "            # with torch.set_grad_enabled(phase == 'train'):\n",
    "                # run the training data through the model\n",
    "            outputs = testModel(inputs)\n",
    "\n",
    "            #calculate the loss of the model\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #Gradient accumulation\n",
    "            loss = loss / accum_iter\n",
    "            loss.backward()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # weights update for each gradient accumulation\n",
    "            if ((i + 1) % accum_iter == 0) or (i + 1 == len(train_loader)):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if ((i + 1) / accum_iter) % 50 == 0:    # record loss and test validation set every 50 gradient accumulations\n",
    "                    testModel.eval()\n",
    "                    v_running_loss = 0.0\n",
    "                    for v, data in enumerate(val_loader):\n",
    "                        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                        v_outputs = testModel(inputs)\n",
    "                        v_loss = criterion(v_outputs, labels)\n",
    "                        v_running_loss += v_loss.item()\n",
    "\n",
    "                    print(\"Time: \", datetime.datetime.now().strftime(\"%H:%M:%S\"), \"\\tepoch: \", epoch+1, \"batch: \", i+1, \"Training loss: \", running_loss, \"Validation loss \", v_running_loss)\n",
    "                    validation_losses.append(v_running_loss)\n",
    "                    training_losses.append(running_loss)\n",
    "                    running_loss = 0.0\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    train_time = end_time - start_time\n",
    "    print(\"Elapsed Training Time: \", datetime.timedelta(seconds = train_time))\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eba46a-1b06-429e-82dd-2316815270a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "\n",
    "#Grid search parameters\n",
    "#dropout=[0.2]\n",
    "batch_size=[4]\n",
    "learn_rate=[1]\n",
    "epochs=[1]\n",
    "\n",
    "\n",
    "it=0\n",
    "#for d in dropout:\n",
    "for e in epochs:\n",
    "    for bs in batch_size:\n",
    "        for lr in learn_rate:\n",
    "\n",
    "            it +=1 \n",
    "            #config = {\"d1\": d,\"lr\": lr,\"batch_size\": bs}\n",
    "            config = {\"epochs\": e, \"lr\": lr, \"batch_size\": bs}\n",
    "            #config = {\"lr\": lr,\"batch_size\": bs}\n",
    "\n",
    "            print(config)\n",
    "            print(\"Train Start. Iteration: \",it)\n",
    "            t_loss,v_loss, end_Ep=train_model(train_dataset, val_dataset,config)\n",
    "            #t_loss,v_loss, acc, end_Ep=train_model(train_dataset, val_dataset,config)\n",
    "\n",
    "            list_row= str(bs) + \",\" + str(lr) + \",\" + str(t_loss) + \",\" + str(v_loss) + \",\" + str(end_Ep)\n",
    "            #list_row= str(bs) + \",\" + str(lr) + \",\" + str(t_loss) + \",\" + str(v_loss) + \",\" + str(end_Ep), + \",\" + str(acc)\n",
    "\n",
    "            t_list=[list_row]  \n",
    "            result_list.append(t_list)\n",
    "\n",
    "\n",
    "with open('opt_results_multimodal.csv','w') as result_file:\n",
    "    wr = csv.writer(result_file, dialect='excel')\n",
    "    wr.writerow(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac2c8c-0e6f-418d-80bb-9dddab3d7f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
