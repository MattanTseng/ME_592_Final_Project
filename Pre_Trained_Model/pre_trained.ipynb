{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '../utils/')\n",
    "from dataset import ChestImage64\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_path = \"../64pxImages/train_labels_64p.csv\"\n",
    "# root_path = '../64pxImages'\n",
    "\n",
    "csv_path = \"../Data/256pxImages/train_labels_256p.csv\"\n",
    "root_path = '../Data/256pxImages'\n",
    "\n",
    "\n",
    "default_transform = ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1.transforms\n",
    "\n",
    "\n",
    "data_transform = Compose([\n",
    "    Resize((64, 64)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61266, 19)\n",
      "label_test:  [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "myCSV = pd.read_csv(csv_path)\n",
    "myCSV['EncodedLabels'] = ''\n",
    "print(myCSV.shape)\n",
    "\n",
    "\n",
    "# for i in range(4, myCSV.shape[1]-1):\n",
    "#     myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + myCSV.iloc[:, i].astype(str) \n",
    "#     if i < myCSV.shape[1]-2:\n",
    "#         myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + \",\" \n",
    "\n",
    "for i in range(4, myCSV.shape[1]-1):\n",
    "    myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + myCSV.iloc[:, i].astype(str) \n",
    "    if i < myCSV.shape[1]-2:\n",
    "        myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + \",\" \n",
    "\n",
    "\n",
    "\n",
    "# myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + \"]\"\n",
    "\n",
    "\n",
    "# We can use the encodedlabels column as our labels for our data\n",
    "\n",
    "# since we are not useing cross attention, pull out only the frontal images. \n",
    "frontalCSV = myCSV[myCSV['Frontal/Lateral'].str.contains(\"Frontal\")]\n",
    "frontalCSV.head()\n",
    "\n",
    "filename = frontalCSV.iloc[1, 0]\n",
    "label_test = frontalCSV['EncodedLabels'].iloc[0]\n",
    "\n",
    "test_path = os.path.join(root_path, filename)\n",
    "\n",
    "\n",
    "label_test = [int(x) for x in label_test.split(\",\")]\n",
    "\n",
    "print(\"label_test: \", label_test)\n",
    "\n",
    "image = io.imread(test_path)\n",
    "print(type(image))\n",
    "image = torch.tensor(image)\n",
    "print(image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>256path</th>\n",
       "      <th>Patient</th>\n",
       "      <th>Study</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>EncodedLabels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frontal/patient00002_study1_Frontal.png</td>\n",
       "      <td>patient00002</td>\n",
       "      <td>study1</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1,1,0,1,1,1,0,0,1,1,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lateral/patient00002_study1_Lateral.png</td>\n",
       "      <td>patient00002</td>\n",
       "      <td>study1</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1,1,0,1,1,1,0,0,1,1,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frontal/patient00004_study1_Frontal.png</td>\n",
       "      <td>patient00004</td>\n",
       "      <td>study1</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lateral/patient00004_study1_Lateral.png</td>\n",
       "      <td>patient00004</td>\n",
       "      <td>study1</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frontal/patient00005_study1_Frontal.png</td>\n",
       "      <td>patient00005</td>\n",
       "      <td>study1</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,1,0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   256path       Patient   Study  \\\n",
       "0  Frontal/patient00002_study1_Frontal.png  patient00002  study1   \n",
       "1  Lateral/patient00002_study1_Lateral.png  patient00002  study1   \n",
       "2  Frontal/patient00004_study1_Frontal.png  patient00004  study1   \n",
       "3  Lateral/patient00004_study1_Lateral.png  patient00004  study1   \n",
       "4  Frontal/patient00005_study1_Frontal.png  patient00005  study1   \n",
       "\n",
       "  Frontal/Lateral  Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  \\\n",
       "0         Frontal                           1             1             1   \n",
       "1         Lateral                           1             1             1   \n",
       "2         Frontal                           0             0             0   \n",
       "3         Lateral                           0             0             0   \n",
       "4         Frontal                           0             0             0   \n",
       "\n",
       "   Lung Lesion  Edema  Consolidation  Pneumonia  Atelectasis  Pneumothorax  \\\n",
       "0            1      0              1          1            1             0   \n",
       "1            1      0              1          1            1             0   \n",
       "2            0      0              0          0            0             0   \n",
       "3            0      0              0          0            0             0   \n",
       "4            0      0              0          0            0             0   \n",
       "\n",
       "   Pleural Effusion  Pleural Other  Fracture  Support Devices  No Finding  \\\n",
       "0                 0              1         1                0           0   \n",
       "1                 0              1         1                0           0   \n",
       "2                 0              0         0                0           1   \n",
       "3                 0              0         0                0           1   \n",
       "4                 0              0         0                1           0   \n",
       "\n",
       "                 EncodedLabels  \n",
       "0  1,1,1,1,0,1,1,1,0,0,1,1,0,0  \n",
       "1  1,1,1,1,0,1,1,1,0,0,1,1,0,0  \n",
       "2  0,0,0,0,0,0,0,0,0,0,0,0,0,1  \n",
       "3  0,0,0,0,0,0,0,0,0,0,0,0,0,1  \n",
       "4  0,0,0,0,0,0,0,0,0,0,0,0,1,0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "myCSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CustomDataset'>\n",
      "Train Length:  21441\n",
      "Validation Length:  3063\n",
      "Test Length:  6126\n",
      "torch.Size([2, 3, 224, 224])\n",
      "torch.float32\n",
      "tensor([[  8.,   8.,   8.,  ...,   7.,   7.,   7.],\n",
      "        [  8.,   9.,   9.,  ...,   9.,   9.,   9.],\n",
      "        [  8.,   8.,   8.,  ..., 119., 116., 117.],\n",
      "        ...,\n",
      "        [204., 206., 207.,  ..., 202., 203., 206.],\n",
      "        [206., 207., 205.,  ..., 194., 197., 201.],\n",
      "        [204., 191., 184.,  ...,  60.,  63.,  61.]])\n",
      "torch.Size([2, 14])\n",
      "19:37:46\n"
     ]
    }
   ],
   "source": [
    "# load up the dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, label_col, transform = None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # get the filename of the image\n",
    "        filename = self.df.iloc[index, 0]\n",
    "        label = self.df[self.label_col].iloc[index]\n",
    "\n",
    "        if type(label) == str:\n",
    "            label = [int(x) for x in label.split(\",\")]\n",
    "\n",
    "        # load the image from disk\n",
    "        path = os.path.join(self.root_dir, filename)\n",
    "        img = io.imread(path)\n",
    "\n",
    "\n",
    "\n",
    "        label = torch.tensor(label)\n",
    "        label = label.float()\n",
    "        img = torch.tensor(img)\n",
    "        img = img.resize_((224, 224))\n",
    "        img = repeat(img, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img = rearrange(img, \"(c h) w -> c h w\", c = 3)\n",
    "        img = img.float()\n",
    "\n",
    "\n",
    "        # if self.transform:\n",
    "            # label = self.transform(label)\n",
    "            # img = self.transform(img)\n",
    "\n",
    "        # return the image and its filename\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "dataset = CustomDataset(frontalCSV, root_dir=root_path, label_col=\"EncodedLabels\", transform=default_transform)\n",
    "\n",
    "print(type(dataset))\n",
    "\n",
    "# split into test train validate\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = int(0.2 * len(dataset))\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(\"Train Length: \", len(train_dataset))\n",
    "print(\"Validation Length: \", len(val_dataset))\n",
    "print(\"Test Length: \", len(test_dataset))\n",
    "\n",
    "batchsize = 2\n",
    "\n",
    "# make three different dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "features, labels = next(iter(train_loader))\n",
    "print(features.size())\n",
    "print(features.dtype)\n",
    "\n",
    "print(features[1, 1, :, :])\n",
    "\n",
    "print(labels.size())\n",
    "print(datetime.datetime.now().strftime(\"%H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about the pretrained models is coming from this link: \n",
    "#https://pytorch.org/vision/master/models.html\n",
    "\n",
    "\n",
    "# just use the default weights. These should yeild the best results\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "num_classes = 14\n",
    "feature_extraction = False\n",
    "model = vit_b_16(weights = weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "learning_rate = 0.1\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "if feature_extraction: \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # change the last layer to have the correct number of classes\n",
    "    model.heads = nn.Sequential(nn.Linear(768, num_classes))\n",
    "    model.heads.requires_grad_ = True\n",
    "else:\n",
    "        model.heads = nn.Sequential(nn.Linear(768, num_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t class_token\n",
      "\t conv_proj.weight\n",
      "\t conv_proj.bias\n",
      "\t encoder.pos_embedding\n",
      "\t encoder.layers.encoder_layer_0.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_0.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_0.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_0.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_1.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_1.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_1.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_1.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_2.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_2.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_2.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_2.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_3.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_3.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_3.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_3.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_4.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_4.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_4.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_4.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_5.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_5.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_5.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_5.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_6.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_6.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_6.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_6.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_6.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_6.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_6.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_6.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_7.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_7.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_7.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_7.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_7.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_7.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_7.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_7.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_8.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_8.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_8.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_8.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_8.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_8.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_8.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_8.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_9.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_9.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_9.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_9.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_9.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_9.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_9.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_9.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_10.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_10.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_10.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_10.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_11.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_11.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_11.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_11.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "\t encoder.ln.weight\n",
      "\t encoder.ln.bias\n",
      "\t heads.0.weight\n",
      "\t heads.0.bias\n",
      "Time:  19:38:14 \tepoch:  1 batch:  100 Training loss:  65.05942663550377 Validation loss  923.3403442502022\n",
      "Time:  19:38:35 \tepoch:  1 batch:  200 Training loss:  59.14471125602722 Validation loss  933.6864548325539\n",
      "Time:  19:38:56 \tepoch:  1 batch:  300 Training loss:  60.22064933180809 Validation loss  905.7597179412842\n",
      "Time:  19:39:17 \tepoch:  1 batch:  400 Training loss:  60.11515748500824 Validation loss  937.9713445007801\n",
      "Time:  19:39:38 \tepoch:  1 batch:  500 Training loss:  61.400497794151306 Validation loss  933.0099033415318\n",
      "Time:  19:39:59 \tepoch:  1 batch:  600 Training loss:  57.68415978550911 Validation loss  971.5099007636309\n",
      "Time:  19:40:20 \tepoch:  1 batch:  700 Training loss:  57.806999534368515 Validation loss  952.8406948298216\n",
      "Time:  19:40:41 \tepoch:  1 batch:  800 Training loss:  60.20363301038742 Validation loss  928.9812195003033\n",
      "Time:  19:41:02 \tepoch:  1 batch:  900 Training loss:  65.01446689665318 Validation loss  935.1204817891121\n",
      "Time:  19:41:23 \tepoch:  1 batch:  1000 Training loss:  60.85210603475571 Validation loss  925.3714251220226\n",
      "Time:  19:41:43 \tepoch:  1 batch:  1100 Training loss:  59.48580211400986 Validation loss  1092.2507535368204\n",
      "Time:  19:42:04 \tepoch:  1 batch:  1200 Training loss:  62.743786081671715 Validation loss  906.6713730096817\n",
      "Time:  19:42:26 \tepoch:  1 batch:  1300 Training loss:  57.05752605199814 Validation loss  894.9236738979816\n",
      "Time:  19:42:47 \tepoch:  1 batch:  1400 Training loss:  56.73964440822601 Validation loss  898.7844414114952\n",
      "Time:  19:43:08 \tepoch:  1 batch:  1500 Training loss:  56.71548902988434 Validation loss  918.8727501034737\n",
      "Time:  19:43:29 \tepoch:  1 batch:  1600 Training loss:  57.0398031771183 Validation loss  902.5625202953815\n",
      "Time:  19:43:50 \tepoch:  1 batch:  1700 Training loss:  57.727644592523575 Validation loss  885.8594298660755\n",
      "Time:  19:44:11 \tepoch:  1 batch:  1800 Training loss:  58.66693416237831 Validation loss  890.1253317594528\n",
      "Time:  19:44:32 \tepoch:  1 batch:  1900 Training loss:  60.05028375983238 Validation loss  1016.3505214750767\n",
      "Time:  19:44:53 \tepoch:  1 batch:  2000 Training loss:  58.01362302899361 Validation loss  918.1727863550186\n",
      "Time:  19:45:14 \tepoch:  1 batch:  2100 Training loss:  57.55050125718117 Validation loss  881.8878425657749\n",
      "Time:  19:45:35 \tepoch:  1 batch:  2200 Training loss:  58.124709874391556 Validation loss  882.9130457639694\n",
      "Time:  19:45:56 \tepoch:  1 batch:  2300 Training loss:  58.35910791158676 Validation loss  1038.5456188321114\n",
      "Time:  19:46:16 \tepoch:  1 batch:  2400 Training loss:  57.95573738217354 Validation loss  896.1537049412727\n",
      "Time:  19:46:38 \tepoch:  1 batch:  2500 Training loss:  57.304653108119965 Validation loss  901.7602941691875\n",
      "Time:  19:46:58 \tepoch:  1 batch:  2600 Training loss:  57.98137637972832 Validation loss  892.8154453039169\n",
      "Time:  19:47:19 \tepoch:  1 batch:  2700 Training loss:  56.45601445436478 Validation loss  885.2028665244579\n",
      "Time:  19:47:40 \tepoch:  1 batch:  2800 Training loss:  55.521744191646576 Validation loss  889.0519851446152\n",
      "Time:  19:48:01 \tepoch:  1 batch:  2900 Training loss:  61.35967102646828 Validation loss  911.0020442306995\n",
      "Time:  19:48:22 \tepoch:  1 batch:  3000 Training loss:  58.48716068267822 Validation loss  884.8678621053696\n",
      "Time:  19:48:43 \tepoch:  1 batch:  3100 Training loss:  58.267051339149475 Validation loss  899.7269442975521\n",
      "Time:  19:49:04 \tepoch:  1 batch:  3200 Training loss:  58.83189716935158 Validation loss  913.872655749321\n",
      "Time:  19:49:25 \tepoch:  1 batch:  3300 Training loss:  59.86703464388847 Validation loss  887.9565233886242\n",
      "Time:  19:49:46 \tepoch:  1 batch:  3400 Training loss:  57.76605752110481 Validation loss  885.3643205165863\n",
      "Time:  19:50:07 \tepoch:  1 batch:  3500 Training loss:  61.9598191678524 Validation loss  894.2176961898804\n",
      "Time:  19:50:28 \tepoch:  1 batch:  3600 Training loss:  59.778289914131165 Validation loss  896.5724418461323\n",
      "Time:  19:50:49 \tepoch:  1 batch:  3700 Training loss:  56.89840844273567 Validation loss  990.3743243813515\n",
      "Time:  19:51:10 \tepoch:  1 batch:  3800 Training loss:  59.73037272691727 Validation loss  885.9496059715748\n",
      "Time:  19:51:30 \tepoch:  1 batch:  3900 Training loss:  59.39004451036453 Validation loss  902.661130219698\n",
      "Time:  19:51:52 \tepoch:  1 batch:  4000 Training loss:  58.04426774382591 Validation loss  880.2645672857761\n",
      "Time:  19:52:12 \tepoch:  1 batch:  4100 Training loss:  56.31796270608902 Validation loss  885.1138352155685\n",
      "Time:  19:52:33 \tepoch:  1 batch:  4200 Training loss:  56.47653701901436 Validation loss  888.1747294962406\n",
      "Time:  19:52:54 \tepoch:  1 batch:  4300 Training loss:  58.311645805835724 Validation loss  883.6919821798801\n",
      "Time:  19:53:15 \tepoch:  1 batch:  4400 Training loss:  58.53548148274422 Validation loss  886.3499936163425\n",
      "Time:  19:53:36 \tepoch:  1 batch:  4500 Training loss:  56.711380153894424 Validation loss  890.0851836502552\n",
      "Time:  19:53:57 \tepoch:  1 batch:  4600 Training loss:  58.385608941316605 Validation loss  884.0863593816757\n",
      "Time:  19:54:18 \tepoch:  1 batch:  4700 Training loss:  58.32187005877495 Validation loss  884.092937707901\n",
      "Time:  19:54:39 \tepoch:  1 batch:  4800 Training loss:  58.096068769693375 Validation loss  881.6166229844093\n",
      "Time:  19:55:00 \tepoch:  1 batch:  4900 Training loss:  57.90447410941124 Validation loss  880.2236501574516\n",
      "Time:  19:55:21 \tepoch:  1 batch:  5000 Training loss:  57.67261824011803 Validation loss  904.6406551003456\n",
      "Time:  19:55:41 \tepoch:  1 batch:  5100 Training loss:  58.60123431682587 Validation loss  881.6946686208248\n",
      "Time:  19:56:02 \tepoch:  1 batch:  5200 Training loss:  56.85153841972351 Validation loss  881.1663962900639\n",
      "Time:  19:56:23 \tepoch:  1 batch:  5300 Training loss:  58.277143478393555 Validation loss  886.2470075190067\n",
      "Time:  19:56:44 \tepoch:  1 batch:  5400 Training loss:  59.65109542012215 Validation loss  892.9615425467491\n",
      "Time:  19:57:05 \tepoch:  1 batch:  5500 Training loss:  59.59166285395622 Validation loss  889.4294532835484\n",
      "Time:  19:57:26 \tepoch:  1 batch:  5600 Training loss:  56.78588750958443 Validation loss  883.7665149867535\n",
      "Time:  19:57:47 \tepoch:  1 batch:  5700 Training loss:  54.955227464437485 Validation loss  888.8801824748516\n",
      "Time:  19:58:08 \tepoch:  1 batch:  5800 Training loss:  57.87548688054085 Validation loss  884.5626540184021\n",
      "Time:  19:58:29 \tepoch:  1 batch:  5900 Training loss:  58.18547296524048 Validation loss  889.4935638606548\n",
      "Time:  19:58:50 \tepoch:  1 batch:  6000 Training loss:  57.700284361839294 Validation loss  893.7408867776394\n",
      "Time:  19:59:10 \tepoch:  1 batch:  6100 Training loss:  56.54240298271179 Validation loss  904.0306279659271\n",
      "Time:  19:59:32 \tepoch:  1 batch:  6200 Training loss:  61.902961760759354 Validation loss  892.5491749942303\n",
      "Time:  19:59:52 \tepoch:  1 batch:  6300 Training loss:  59.37753275036812 Validation loss  900.4626104533672\n",
      "Time:  20:00:13 \tepoch:  1 batch:  6400 Training loss:  57.73982697725296 Validation loss  996.1676792651415\n",
      "Time:  20:00:34 \tepoch:  1 batch:  6500 Training loss:  59.05995362997055 Validation loss  890.0338213741779\n",
      "Time:  20:00:55 \tepoch:  1 batch:  6600 Training loss:  57.316176533699036 Validation loss  889.8488411903381\n",
      "Time:  20:01:16 \tepoch:  1 batch:  6700 Training loss:  59.39044043421745 Validation loss  887.1220293343067\n",
      "Time:  20:01:37 \tepoch:  1 batch:  6800 Training loss:  56.572377264499664 Validation loss  901.3405365049839\n",
      "Time:  20:01:58 \tepoch:  1 batch:  6900 Training loss:  59.31130167841911 Validation loss  891.9854967594147\n",
      "Time:  20:02:19 \tepoch:  1 batch:  7000 Training loss:  58.13948392868042 Validation loss  883.885111361742\n",
      "Time:  20:02:40 \tepoch:  1 batch:  7100 Training loss:  59.694001108407974 Validation loss  881.671614587307\n",
      "Time:  20:03:01 \tepoch:  1 batch:  7200 Training loss:  58.64134928584099 Validation loss  884.8130644857883\n",
      "Time:  20:03:22 \tepoch:  1 batch:  7300 Training loss:  59.365345150232315 Validation loss  882.3329797685146\n",
      "Time:  20:03:42 \tepoch:  1 batch:  7400 Training loss:  54.94055554270744 Validation loss  897.033333003521\n",
      "Time:  20:04:03 \tepoch:  1 batch:  7500 Training loss:  59.26671093702316 Validation loss  882.3503337204456\n",
      "Time:  20:04:24 \tepoch:  1 batch:  7600 Training loss:  56.22270303964615 Validation loss  888.705923050642\n",
      "Time:  20:04:45 \tepoch:  1 batch:  7700 Training loss:  53.78563465178013 Validation loss  887.0578499734402\n",
      "Time:  20:05:06 \tepoch:  1 batch:  7800 Training loss:  59.53911945223808 Validation loss  882.3564494252205\n",
      "Time:  20:05:27 \tepoch:  1 batch:  7900 Training loss:  59.5251139998436 Validation loss  924.8616116642952\n",
      "Time:  20:05:47 \tepoch:  1 batch:  8000 Training loss:  56.3105229139328 Validation loss  890.4592839479446\n",
      "Time:  20:06:08 \tepoch:  1 batch:  8100 Training loss:  59.250365763902664 Validation loss  883.4406406581402\n",
      "Time:  20:06:29 \tepoch:  1 batch:  8200 Training loss:  57.34860146045685 Validation loss  891.7754702270031\n",
      "Time:  20:06:50 \tepoch:  1 batch:  8300 Training loss:  58.232930064201355 Validation loss  930.8827937543392\n",
      "Time:  20:07:11 \tepoch:  1 batch:  8400 Training loss:  58.02954325079918 Validation loss  882.9775948524475\n",
      "Time:  20:07:31 \tepoch:  1 batch:  8500 Training loss:  58.89512252807617 Validation loss  950.5564204752445\n",
      "Time:  20:07:52 \tepoch:  1 batch:  8600 Training loss:  57.92840030789375 Validation loss  913.6289879977703\n",
      "Time:  20:08:13 \tepoch:  1 batch:  8700 Training loss:  55.95357185602188 Validation loss  910.2532761096954\n",
      "Time:  20:08:34 \tepoch:  1 batch:  8800 Training loss:  57.26576176285744 Validation loss  888.1806013286114\n",
      "Time:  20:08:55 \tepoch:  1 batch:  8900 Training loss:  57.07294972240925 Validation loss  881.2295853793621\n",
      "Time:  20:09:16 \tepoch:  1 batch:  9000 Training loss:  57.4146271944046 Validation loss  883.015181273222\n",
      "Time:  20:09:37 \tepoch:  1 batch:  9100 Training loss:  57.77508944272995 Validation loss  881.2190442085266\n",
      "Time:  20:09:58 \tepoch:  1 batch:  9200 Training loss:  55.315456837415695 Validation loss  886.012979477644\n",
      "Time:  20:10:19 \tepoch:  1 batch:  9300 Training loss:  58.34053348004818 Validation loss  885.270335406065\n",
      "Time:  20:10:39 \tepoch:  1 batch:  9400 Training loss:  57.72051975131035 Validation loss  909.8515285551548\n",
      "Time:  20:11:00 \tepoch:  1 batch:  9500 Training loss:  58.40152150392532 Validation loss  882.1631460785866\n",
      "Time:  20:11:21 \tepoch:  1 batch:  9600 Training loss:  58.14574620127678 Validation loss  882.0162005126476\n",
      "Time:  20:11:42 \tepoch:  1 batch:  9700 Training loss:  56.55723416805267 Validation loss  882.4922098815441\n",
      "Time:  20:12:02 \tepoch:  1 batch:  9800 Training loss:  57.991398483514786 Validation loss  881.5081228613853\n",
      "Time:  20:12:23 \tepoch:  1 batch:  9900 Training loss:  59.01827600598335 Validation loss  890.8447123467922\n",
      "Time:  20:12:44 \tepoch:  1 batch:  10000 Training loss:  58.80642148852348 Validation loss  887.7913834452629\n",
      "Time:  20:13:05 \tepoch:  1 batch:  10100 Training loss:  58.927387446165085 Validation loss  884.2247785627842\n",
      "Time:  20:13:26 \tepoch:  1 batch:  10200 Training loss:  57.622892588377 Validation loss  883.3909458518028\n",
      "Time:  20:13:47 \tepoch:  1 batch:  10300 Training loss:  57.34500330686569 Validation loss  880.3968072831631\n",
      "Time:  20:14:08 \tepoch:  1 batch:  10400 Training loss:  56.5686554312706 Validation loss  880.9665374457836\n",
      "Time:  20:14:28 \tepoch:  1 batch:  10500 Training loss:  58.171258598566055 Validation loss  882.8049428164959\n",
      "Time:  20:14:49 \tepoch:  1 batch:  10600 Training loss:  57.63053458929062 Validation loss  880.475149333477\n",
      "Time:  20:15:10 \tepoch:  1 batch:  10700 Training loss:  56.927184253931046 Validation loss  882.6420400738716\n",
      "Elapsed Training Time:  0:37:21.715704\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# now let's train this thing. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# make sure the model is running on the GPU if its available\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "params_to_update =model.parameters()\n",
    "\n",
    "# print out a list of the parameters we are training\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(model500k.parameters(), lr=learning_rate)\n",
    "\n",
    "# start a clock \n",
    "start_time = time.time()\n",
    "\n",
    "# create empty arrays to hold the loss results\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(epochs):\n",
    "    phase = 'train'\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # ugh. This is gross. I should have done this step at the beginning for all of the datasets. \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"Here's the size of the inputs: \", inputs.size())\n",
    "        # with torch.set_grad_enabled(phase == 'train'):\n",
    "            # run the training data through the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        #calculate the loss of the model\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 99:    # record loss and test validation set\n",
    "            model.eval()\n",
    "            v_running_loss = 0.0\n",
    "            for v, vdata in enumerate(val_loader):\n",
    "                v_inputs, v_labels = vdata[0].to(device), vdata[1].to(device)\n",
    "                v_outputs = model(v_inputs)\n",
    "                v_loss = criterion(v_outputs, v_labels)\n",
    "                v_running_loss += v_loss.item()\n",
    "\n",
    "            print(\"Time: \", datetime.datetime.now().strftime(\"%H:%M:%S\"), \"\\tepoch: \", epoch+1, \"batch: \", i+1, \"Training loss: \", running_loss, \"Validation loss \", v_running_loss)\n",
    "            validation_losses.append(v_running_loss)\n",
    "            training_losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "        \n",
    "\n",
    "        # once the validation has been completed, update the model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # set model back to training mode\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "train_time = end_time - start_time\n",
    "print(\"Elapsed Training Time: \", datetime.timedelta(seconds = train_time))\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb795ac2e20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(type(outputs))\n",
    "#print(outputs.size())\n",
    "#print(outputs[1, :])\n",
    "#print(labels.size())\n",
    "#print(labels[1, :])\n",
    "#print(outputs.dtype)\n",
    "#print(labels.dtype)\n",
    "#print(loss)\n",
    "\n",
    "#print(training_losses)\n",
    "\n",
    "plt.title(\"Train and Val Loss\")\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_488317/3478567511.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mvoutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mpredictedClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mactualClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "\n",
    "# now that the training has been complete, let's run through this baby with our test set.\n",
    "\n",
    "# put the model back in evaluation mode. \n",
    "model.eval()\n",
    "incorrect_combination = []\n",
    "\n",
    "# now run the validation set once more to pull out the mis-typed images. \n",
    "\n",
    "incorrect_classification = np.ndarray(1)\n",
    "incorrect_label = np.ndarray(1)\n",
    "\n",
    "for v, vdata in enumerate(val_loader):\n",
    "    vinputs, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "    voutputs = model(vinputs)\n",
    "\n",
    "    for i in range(len(voutputs)):\n",
    "        if voutputs[i].argmax().cpu().numpy() != vlabels[i].cpu().numpy():\n",
    "            predictedClass = int(voutputs[i].argmax().cpu().numpy())\n",
    "            actualClass = int(vlabels[i].cpu().numpy())\n",
    "            myTuple = (predictedClass, actualClass)\n",
    "            incorrect_combination.append(myTuple)\n",
    "            # incorrect_classification = np.append(incorrect_classification, voutputs[i].argmax().cpu().numpy())\n",
    "            # incorrect_label = np.append(incorrect_label, vlabels[i].cpu().numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Counter from the collections module to count the occurrences of each item\n",
    "counter = collections.Counter(incorrect_combination)\n",
    "\n",
    "# Find the most common item and its count\n",
    "most_common_item, count = counter.most_common(1)[0]\n",
    "\n",
    "print(f\"The most common misclassification occured: {count} times\")\n",
    "print(f\"Categorized: {catagories[most_common_item[1]]}\")\n",
    "print(f\"As: {catagories[most_common_item[0]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
