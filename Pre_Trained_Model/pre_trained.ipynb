{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '../utils/')\n",
    "from dataset import ChestImage64\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_path = \"../64pxImages/train_labels_64p.csv\"\n",
    "# root_path = '../64pxImages'\n",
    "\n",
    "csv_path = \"../Data/256pxImages/train_labels_256p.csv\"\n",
    "root_path = '../Data/256pxImages'\n",
    "\n",
    "\n",
    "default_transform = ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1.transforms\n",
    "\n",
    "\n",
    "data_transform = Compose([\n",
    "    Resize((64, 64)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61266, 19)\n",
      "label_test:  [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "myCSV = pd.read_csv(csv_path)\n",
    "myCSV['EncodedLabels'] = ''\n",
    "print(myCSV.shape)\n",
    "\n",
    "\n",
    "# for i in range(4, myCSV.shape[1]-1):\n",
    "#     myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + myCSV.iloc[:, i].astype(str) \n",
    "#     if i < myCSV.shape[1]-2:\n",
    "#         myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + \",\" \n",
    "\n",
    "for i in range(4, myCSV.shape[1]-1):\n",
    "    myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + myCSV.iloc[:, i].astype(str) \n",
    "    if i < myCSV.shape[1]-2:\n",
    "        myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + \",\" \n",
    "\n",
    "\n",
    "\n",
    "# myCSV['EncodedLabels'] = myCSV['EncodedLabels'].astype(str) + \"]\"\n",
    "\n",
    "\n",
    "# We can use the encodedlabels column as our labels for our data\n",
    "\n",
    "# since we are not useing cross attention, pull out only the frontal images. \n",
    "frontalCSV = myCSV[myCSV['Frontal/Lateral'].str.contains(\"Frontal\")]\n",
    "frontalCSV.head()\n",
    "\n",
    "filename = frontalCSV.iloc[1, 0]\n",
    "label_test = frontalCSV['EncodedLabels'].iloc[0]\n",
    "\n",
    "test_path = os.path.join(root_path, filename)\n",
    "\n",
    "\n",
    "label_test = [int(x) for x in label_test.split(\",\")]\n",
    "\n",
    "print(\"label_test: \", label_test)\n",
    "\n",
    "image = io.imread(test_path)\n",
    "print(type(image))\n",
    "image = torch.tensor(image)\n",
    "print(image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>256path</th>\n",
       "      <th>Patient</th>\n",
       "      <th>Study</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>EncodedLabels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frontal/patient00002_study1_Frontal.png</td>\n",
       "      <td>patient00002</td>\n",
       "      <td>study1</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1,1,0,1,1,1,0,0,1,1,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lateral/patient00002_study1_Lateral.png</td>\n",
       "      <td>patient00002</td>\n",
       "      <td>study1</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1,1,1,0,1,1,1,0,0,1,1,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frontal/patient00004_study1_Frontal.png</td>\n",
       "      <td>patient00004</td>\n",
       "      <td>study1</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lateral/patient00004_study1_Lateral.png</td>\n",
       "      <td>patient00004</td>\n",
       "      <td>study1</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frontal/patient00005_study1_Frontal.png</td>\n",
       "      <td>patient00005</td>\n",
       "      <td>study1</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,1,0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   256path       Patient   Study  \\\n",
       "0  Frontal/patient00002_study1_Frontal.png  patient00002  study1   \n",
       "1  Lateral/patient00002_study1_Lateral.png  patient00002  study1   \n",
       "2  Frontal/patient00004_study1_Frontal.png  patient00004  study1   \n",
       "3  Lateral/patient00004_study1_Lateral.png  patient00004  study1   \n",
       "4  Frontal/patient00005_study1_Frontal.png  patient00005  study1   \n",
       "\n",
       "  Frontal/Lateral  Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  \\\n",
       "0         Frontal                           1             1             1   \n",
       "1         Lateral                           1             1             1   \n",
       "2         Frontal                           0             0             0   \n",
       "3         Lateral                           0             0             0   \n",
       "4         Frontal                           0             0             0   \n",
       "\n",
       "   Lung Lesion  Edema  Consolidation  Pneumonia  Atelectasis  Pneumothorax  \\\n",
       "0            1      0              1          1            1             0   \n",
       "1            1      0              1          1            1             0   \n",
       "2            0      0              0          0            0             0   \n",
       "3            0      0              0          0            0             0   \n",
       "4            0      0              0          0            0             0   \n",
       "\n",
       "   Pleural Effusion  Pleural Other  Fracture  Support Devices  No Finding  \\\n",
       "0                 0              1         1                0           0   \n",
       "1                 0              1         1                0           0   \n",
       "2                 0              0         0                0           1   \n",
       "3                 0              0         0                0           1   \n",
       "4                 0              0         0                1           0   \n",
       "\n",
       "                 EncodedLabels  \n",
       "0  1,1,1,1,0,1,1,1,0,0,1,1,0,0  \n",
       "1  1,1,1,1,0,1,1,1,0,0,1,1,0,0  \n",
       "2  0,0,0,0,0,0,0,0,0,0,0,0,0,1  \n",
       "3  0,0,0,0,0,0,0,0,0,0,0,0,0,1  \n",
       "4  0,0,0,0,0,0,0,0,0,0,0,0,1,0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "myCSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CustomDataset'>\n",
      "Train Length:  21441\n",
      "Validation Length:  3063\n",
      "Test Length:  6126\n",
      "torch.Size([2, 3, 224, 224])\n",
      "torch.float32\n",
      "tensor([[ 30.,  27.,  20.,  ...,  14.,  15.,  15.],\n",
      "        [ 14.,  13.,  13.,  ...,  10.,  10.,  10.],\n",
      "        [ 10.,  10.,  10.,  ...,  12.,  13.,  12.],\n",
      "        ...,\n",
      "        [218., 221., 224.,  ..., 197., 156., 159.],\n",
      "        [147., 134., 145.,  ..., 130., 133., 143.],\n",
      "        [144., 147., 154.,  ...,  31.,  30.,  31.]])\n",
      "torch.Size([2, 14])\n",
      "01:56:11\n"
     ]
    }
   ],
   "source": [
    "# load up the dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, label_col, transform = None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # get the filename of the image\n",
    "        filename = self.df.iloc[index, 0]\n",
    "        label = self.df[self.label_col].iloc[index]\n",
    "\n",
    "        if type(label) == str:\n",
    "            label = [int(x) for x in label.split(\",\")]\n",
    "\n",
    "        # load the image from disk\n",
    "        path = os.path.join(self.root_dir, filename)\n",
    "        img = io.imread(path)\n",
    "\n",
    "\n",
    "\n",
    "        label = torch.tensor(label)\n",
    "        label = label.float()\n",
    "        img = torch.tensor(img)\n",
    "        img = img.resize_((224, 224))\n",
    "        img = repeat(img, \"h w -> (repeat h) w\", repeat = 3)\n",
    "        img = rearrange(img, \"(c h) w -> c h w\", c = 3)\n",
    "        img = img.float()\n",
    "\n",
    "\n",
    "        # if self.transform:\n",
    "            # label = self.transform(label)\n",
    "            # img = self.transform(img)\n",
    "\n",
    "        # return the image and its filename\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "dataset = CustomDataset(frontalCSV, root_dir=root_path, label_col=\"EncodedLabels\", transform=default_transform)\n",
    "\n",
    "print(type(dataset))\n",
    "\n",
    "# split into test train validate\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = int(0.2 * len(dataset))\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(\"Train Length: \", len(train_dataset))\n",
    "print(\"Validation Length: \", len(val_dataset))\n",
    "print(\"Test Length: \", len(test_dataset))\n",
    "\n",
    "batchsize = 2\n",
    "\n",
    "# make three different dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "features, labels = next(iter(train_loader))\n",
    "print(features.size())\n",
    "print(features.dtype)\n",
    "\n",
    "print(features[1, 1, :, :])\n",
    "\n",
    "print(labels.size())\n",
    "print(datetime.datetime.now().strftime(\"%H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about the pretrained models is coming from this link: \n",
    "#https://pytorch.org/vision/master/models.html\n",
    "\n",
    "\n",
    "# just use the default weights. These should yeild the best results\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "num_classes = 14\n",
    "feature_extraction = False\n",
    "model = vit_b_16(weights = weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "learning_rate = 0.1\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "if feature_extraction: \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # change the last layer to have the correct number of classes\n",
    "    model.heads = nn.Sequential(nn.Linear(768, num_classes))\n",
    "    model.heads.requires_grad_ = True\n",
    "else:\n",
    "        model.heads = nn.Sequential(nn.Linear(768, num_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t class_token\n",
      "\t conv_proj.weight\n",
      "\t conv_proj.bias\n",
      "\t encoder.pos_embedding\n",
      "\t encoder.layers.encoder_layer_0.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_0.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_0.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_0.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_1.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_1.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_1.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_1.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_2.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_2.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_2.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_2.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_3.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_3.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_3.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_3.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_4.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_4.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_4.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_4.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_5.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_5.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_5.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_5.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_6.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_6.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_6.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_6.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_6.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_6.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_6.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_6.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_7.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_7.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_7.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_7.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_7.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_7.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_7.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_7.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_8.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_8.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_8.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_8.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_8.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_8.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_8.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_8.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_9.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_9.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_9.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_9.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_9.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_9.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_9.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_9.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_10.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_10.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_10.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_10.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "\t encoder.layers.encoder_layer_11.ln_1.weight\n",
      "\t encoder.layers.encoder_layer_11.ln_1.bias\n",
      "\t encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "\t encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "\t encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "\t encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "\t encoder.layers.encoder_layer_11.ln_2.weight\n",
      "\t encoder.layers.encoder_layer_11.ln_2.bias\n",
      "\t encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "\t encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "\t encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "\t encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "\t encoder.ln.weight\n",
      "\t encoder.ln.bias\n",
      "\t heads.0.weight\n",
      "\t heads.0.bias\n",
      "Time:  01:56:42 \tepoch:  1 batch:  100 Training loss:  72.97432117536664 Validation loss  899.528385668993\n",
      "Time:  01:57:03 \tepoch:  1 batch:  200 Training loss:  59.749889105558395 Validation loss  897.1621498465538\n",
      "Time:  01:57:24 \tepoch:  1 batch:  300 Training loss:  58.9669035077095 Validation loss  887.0153593420982\n",
      "Time:  01:57:45 \tepoch:  1 batch:  400 Training loss:  59.143051356077194 Validation loss  914.3079766333103\n",
      "Time:  01:58:05 \tepoch:  1 batch:  500 Training loss:  60.590465158224106 Validation loss  958.7265941798687\n",
      "Time:  01:58:26 \tepoch:  1 batch:  600 Training loss:  59.68353581428528 Validation loss  893.7012346982956\n",
      "Time:  01:58:47 \tepoch:  1 batch:  700 Training loss:  58.77720320224762 Validation loss  903.7308954000473\n",
      "Time:  01:59:08 \tepoch:  1 batch:  800 Training loss:  55.926979035139084 Validation loss  1021.8210247308016\n",
      "Time:  01:59:29 \tepoch:  1 batch:  900 Training loss:  58.26868990063667 Validation loss  884.9229559600353\n",
      "Time:  01:59:50 \tepoch:  1 batch:  1000 Training loss:  58.4886469244957 Validation loss  883.7422687411308\n",
      "Time:  02:00:10 \tepoch:  1 batch:  1100 Training loss:  56.150991439819336 Validation loss  890.7652455866337\n",
      "Time:  02:00:31 \tepoch:  1 batch:  1200 Training loss:  60.45528793334961 Validation loss  893.6567915678024\n",
      "Time:  02:00:52 \tepoch:  1 batch:  1300 Training loss:  61.5988404750824 Validation loss  904.7794865369797\n",
      "Time:  02:01:13 \tepoch:  1 batch:  1400 Training loss:  57.860029488801956 Validation loss  887.0413126349449\n",
      "Time:  02:01:34 \tepoch:  1 batch:  1500 Training loss:  58.15864133834839 Validation loss  904.7189516723156\n",
      "Time:  02:01:55 \tepoch:  1 batch:  1600 Training loss:  61.886585891246796 Validation loss  883.502309679985\n",
      "Time:  02:02:16 \tepoch:  1 batch:  1700 Training loss:  58.92435124516487 Validation loss  894.4215249717236\n",
      "Time:  02:02:37 \tepoch:  1 batch:  1800 Training loss:  56.283386185765266 Validation loss  882.6020097136497\n",
      "Time:  02:02:57 \tepoch:  1 batch:  1900 Training loss:  56.68103897571564 Validation loss  879.7503884732723\n",
      "Time:  02:03:18 \tepoch:  1 batch:  2000 Training loss:  57.09231722354889 Validation loss  881.1426488161087\n",
      "Time:  02:03:39 \tepoch:  1 batch:  2100 Training loss:  56.94809889793396 Validation loss  926.860447883606\n",
      "Time:  02:03:59 \tepoch:  1 batch:  2200 Training loss:  57.894347846508026 Validation loss  883.2473317086697\n",
      "Time:  02:04:21 \tepoch:  1 batch:  2300 Training loss:  57.823499739170074 Validation loss  887.4509384334087\n",
      "Time:  02:04:42 \tepoch:  1 batch:  2400 Training loss:  57.206707805395126 Validation loss  890.7714827656746\n",
      "Time:  02:05:03 \tepoch:  1 batch:  2500 Training loss:  59.78379359841347 Validation loss  893.2940277159214\n",
      "Time:  02:05:24 \tepoch:  1 batch:  2600 Training loss:  57.450956642627716 Validation loss  883.4318284094334\n",
      "Time:  02:05:44 \tepoch:  1 batch:  2700 Training loss:  58.49219360947609 Validation loss  885.2263999581337\n",
      "Time:  02:06:05 \tepoch:  1 batch:  2800 Training loss:  54.46204388141632 Validation loss  887.0200532972813\n",
      "Time:  02:06:26 \tepoch:  1 batch:  2900 Training loss:  56.940737038850784 Validation loss  877.8259008824825\n",
      "Time:  02:06:47 \tepoch:  1 batch:  3000 Training loss:  57.09835824370384 Validation loss  881.8350325226784\n",
      "Time:  02:07:08 \tepoch:  1 batch:  3100 Training loss:  57.156465977430344 Validation loss  878.5169999599457\n",
      "Time:  02:07:29 \tepoch:  1 batch:  3200 Training loss:  60.04399773478508 Validation loss  886.1323426365852\n",
      "Time:  02:07:50 \tepoch:  1 batch:  3300 Training loss:  57.86805707216263 Validation loss  891.2123223543167\n",
      "Time:  02:08:11 \tepoch:  1 batch:  3400 Training loss:  59.39592504501343 Validation loss  893.766808360815\n",
      "Time:  02:08:32 \tepoch:  1 batch:  3500 Training loss:  55.574916273355484 Validation loss  894.1190013289452\n",
      "Time:  02:08:53 \tepoch:  1 batch:  3600 Training loss:  57.09273678064346 Validation loss  958.3755087256432\n",
      "Time:  02:09:13 \tepoch:  1 batch:  3700 Training loss:  59.00807425379753 Validation loss  998.8009355515242\n",
      "Time:  02:09:34 \tepoch:  1 batch:  3800 Training loss:  61.211486384272575 Validation loss  882.28142786026\n",
      "Time:  02:09:55 \tepoch:  1 batch:  3900 Training loss:  60.23998910188675 Validation loss  881.6298232078552\n",
      "Time:  02:10:16 \tepoch:  1 batch:  4000 Training loss:  58.0601324737072 Validation loss  888.5563128888607\n",
      "Time:  02:10:37 \tepoch:  1 batch:  4100 Training loss:  60.46316233277321 Validation loss  880.53519064188\n",
      "Time:  02:10:58 \tepoch:  1 batch:  4200 Training loss:  57.07230669260025 Validation loss  886.8720695078373\n",
      "Time:  02:11:19 \tepoch:  1 batch:  4300 Training loss:  58.173218965530396 Validation loss  889.1599256694317\n",
      "Time:  02:11:39 \tepoch:  1 batch:  4400 Training loss:  54.65068234503269 Validation loss  892.7988268435001\n",
      "Time:  02:12:00 \tepoch:  1 batch:  4500 Training loss:  58.76123848557472 Validation loss  886.5324530005455\n",
      "Time:  02:12:21 \tepoch:  1 batch:  4600 Training loss:  57.82551458477974 Validation loss  879.3002556860447\n",
      "Time:  02:12:42 \tepoch:  1 batch:  4700 Training loss:  59.38380452990532 Validation loss  881.8660407066345\n",
      "Time:  02:13:03 \tepoch:  1 batch:  4800 Training loss:  56.37884563207626 Validation loss  894.2425193786621\n",
      "Time:  02:13:24 \tepoch:  1 batch:  4900 Training loss:  55.71633863449097 Validation loss  878.5619163811207\n",
      "Time:  02:13:44 \tepoch:  1 batch:  5000 Training loss:  56.061207979917526 Validation loss  878.2913031578064\n",
      "Time:  02:14:05 \tepoch:  1 batch:  5100 Training loss:  59.07647126913071 Validation loss  879.3177468776703\n",
      "Time:  02:14:26 \tepoch:  1 batch:  5200 Training loss:  57.00138992071152 Validation loss  893.5016498267651\n",
      "Time:  02:14:47 \tepoch:  1 batch:  5300 Training loss:  59.92540308833122 Validation loss  933.6298822760582\n",
      "Time:  02:15:08 \tepoch:  1 batch:  5400 Training loss:  57.82962965965271 Validation loss  881.7355310916901\n",
      "Time:  02:15:29 \tepoch:  1 batch:  5500 Training loss:  57.627655893564224 Validation loss  879.141424447298\n",
      "Time:  02:15:50 \tepoch:  1 batch:  5600 Training loss:  59.186198472976685 Validation loss  889.4919821023941\n",
      "Time:  02:16:11 \tepoch:  1 batch:  5700 Training loss:  56.60354931652546 Validation loss  880.2108679711819\n",
      "Time:  02:16:32 \tepoch:  1 batch:  5800 Training loss:  59.98620343208313 Validation loss  878.7828065156937\n",
      "Time:  02:16:53 \tepoch:  1 batch:  5900 Training loss:  59.82247641682625 Validation loss  887.1926307678223\n",
      "Time:  02:17:14 \tepoch:  1 batch:  6000 Training loss:  58.33775523304939 Validation loss  887.2680561840534\n",
      "Time:  02:17:34 \tepoch:  1 batch:  6100 Training loss:  56.48247069120407 Validation loss  885.5105309188366\n",
      "Time:  02:17:55 \tepoch:  1 batch:  6200 Training loss:  55.362006932497025 Validation loss  883.7744162380695\n",
      "Time:  02:18:16 \tepoch:  1 batch:  6300 Training loss:  56.42452645301819 Validation loss  886.7413948774338\n",
      "Time:  02:18:37 \tepoch:  1 batch:  6400 Training loss:  57.27478930354118 Validation loss  879.0042540431023\n",
      "Time:  02:18:57 \tepoch:  1 batch:  6500 Training loss:  56.13014328479767 Validation loss  902.3535962998867\n",
      "Time:  02:19:18 \tepoch:  1 batch:  6600 Training loss:  58.2111377120018 Validation loss  883.6717996895313\n",
      "Time:  02:19:39 \tepoch:  1 batch:  6700 Training loss:  56.844317853450775 Validation loss  880.9847167134285\n",
      "Time:  02:20:00 \tepoch:  1 batch:  6800 Training loss:  56.07084199786186 Validation loss  882.2021608054638\n",
      "Time:  02:20:21 \tepoch:  1 batch:  6900 Training loss:  58.17220366001129 Validation loss  893.5455810725689\n",
      "Time:  02:20:41 \tepoch:  1 batch:  7000 Training loss:  58.193832367658615 Validation loss  879.207178235054\n",
      "Time:  02:21:02 \tepoch:  1 batch:  7100 Training loss:  56.42547497153282 Validation loss  889.0864128470421\n",
      "Time:  02:21:23 \tepoch:  1 batch:  7200 Training loss:  54.28242000937462 Validation loss  878.5207191705704\n",
      "Time:  02:21:44 \tepoch:  1 batch:  7300 Training loss:  59.42073741555214 Validation loss  880.6790131926537\n",
      "Time:  02:22:04 \tepoch:  1 batch:  7400 Training loss:  56.5644106566906 Validation loss  881.4802658557892\n",
      "Time:  02:22:25 \tepoch:  1 batch:  7500 Training loss:  58.97125607728958 Validation loss  882.6402432918549\n",
      "Time:  02:22:46 \tepoch:  1 batch:  7600 Training loss:  55.93437123298645 Validation loss  880.12526217103\n",
      "Time:  02:23:07 \tepoch:  1 batch:  7700 Training loss:  55.48685139417648 Validation loss  880.7969958484173\n",
      "Time:  02:23:28 \tepoch:  1 batch:  7800 Training loss:  58.74426847696304 Validation loss  880.1527931392193\n",
      "Time:  02:23:49 \tepoch:  1 batch:  7900 Training loss:  60.52644118666649 Validation loss  880.8633262813091\n",
      "Time:  02:24:09 \tepoch:  1 batch:  8000 Training loss:  56.08459249138832 Validation loss  878.6902563273907\n",
      "Time:  02:24:30 \tepoch:  1 batch:  8100 Training loss:  56.26862183213234 Validation loss  883.5161634385586\n",
      "Time:  02:24:51 \tepoch:  1 batch:  8200 Training loss:  58.38730654120445 Validation loss  970.0198746919632\n",
      "Time:  02:25:12 \tepoch:  1 batch:  8300 Training loss:  60.143097043037415 Validation loss  886.2979765236378\n",
      "Time:  02:25:32 \tepoch:  1 batch:  8400 Training loss:  57.43340918421745 Validation loss  883.2224974036217\n",
      "Time:  02:25:53 \tepoch:  1 batch:  8500 Training loss:  56.10275927186012 Validation loss  877.8386934101582\n",
      "Time:  02:26:14 \tepoch:  1 batch:  8600 Training loss:  55.46966478228569 Validation loss  882.0870326459408\n",
      "Time:  02:26:35 \tepoch:  1 batch:  8700 Training loss:  56.85763218998909 Validation loss  879.2993924021721\n",
      "Time:  02:26:56 \tepoch:  1 batch:  8800 Training loss:  55.84993824362755 Validation loss  883.9948402643204\n",
      "Time:  02:27:17 \tepoch:  1 batch:  8900 Training loss:  59.40574765205383 Validation loss  904.9625568985939\n",
      "Time:  02:27:38 \tepoch:  1 batch:  9000 Training loss:  59.305359184741974 Validation loss  885.7032293081284\n",
      "Time:  02:27:59 \tepoch:  1 batch:  9100 Training loss:  57.73957258462906 Validation loss  900.827381670475\n",
      "Time:  02:28:20 \tepoch:  1 batch:  9200 Training loss:  55.86678984761238 Validation loss  888.5239166617393\n",
      "Time:  02:28:40 \tepoch:  1 batch:  9300 Training loss:  57.71630245447159 Validation loss  880.1045396327972\n",
      "Time:  02:29:02 \tepoch:  1 batch:  9400 Training loss:  58.377527356147766 Validation loss  881.5435516536236\n",
      "Time:  02:29:22 \tepoch:  1 batch:  9500 Training loss:  57.23835128545761 Validation loss  879.3499093949795\n",
      "Time:  02:29:43 \tepoch:  1 batch:  9600 Training loss:  55.41810467839241 Validation loss  879.706221729517\n",
      "Time:  02:30:04 \tepoch:  1 batch:  9700 Training loss:  57.39920210838318 Validation loss  889.4965123236179\n",
      "Time:  02:30:25 \tepoch:  1 batch:  9800 Training loss:  58.593766897916794 Validation loss  881.0277687609196\n",
      "Time:  02:30:46 \tepoch:  1 batch:  9900 Training loss:  56.543561428785324 Validation loss  883.4256036281586\n",
      "Time:  02:31:06 \tepoch:  1 batch:  10000 Training loss:  57.57222110033035 Validation loss  890.5626060962677\n",
      "Time:  02:31:27 \tepoch:  1 batch:  10100 Training loss:  54.53975349664688 Validation loss  889.344319164753\n",
      "Time:  02:31:48 \tepoch:  1 batch:  10200 Training loss:  58.80133578181267 Validation loss  887.0904660522938\n",
      "Time:  02:32:09 \tepoch:  1 batch:  10300 Training loss:  57.445499420166016 Validation loss  885.3182647228241\n",
      "Time:  02:32:30 \tepoch:  1 batch:  10400 Training loss:  56.863364815711975 Validation loss  879.7869262993336\n",
      "Time:  02:32:51 \tepoch:  1 batch:  10500 Training loss:  59.262269884347916 Validation loss  884.6268297433853\n",
      "Time:  02:33:12 \tepoch:  1 batch:  10600 Training loss:  56.30932426452637 Validation loss  889.9944302141666\n",
      "Time:  02:33:33 \tepoch:  1 batch:  10700 Training loss:  58.203675866127014 Validation loss  883.1436405181885\n",
      "Elapsed Training Time:  0:37:16.319470\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# now let's train this thing. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# make sure the model is running on the GPU if its available\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "params_to_update =model.parameters()\n",
    "\n",
    "# print out a list of the parameters we are training\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(model500k.parameters(), lr=learning_rate)\n",
    "\n",
    "# start a clock \n",
    "start_time = time.time()\n",
    "\n",
    "# create empty arrays to hold the loss results\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(epochs):\n",
    "    phase = 'train'\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # ugh. This is gross. I should have done this step at the beginning for all of the datasets. \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"Here's the size of the inputs: \", inputs.size())\n",
    "        # with torch.set_grad_enabled(phase == 'train'):\n",
    "            # run the training data through the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        #calculate the loss of the model\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 99:    # record loss and test validation set\n",
    "            model.eval()\n",
    "            v_running_loss = 0.0\n",
    "            for v, vdata in enumerate(val_loader):\n",
    "                v_inputs, v_labels = vdata[0].to(device), vdata[1].to(device)\n",
    "                v_outputs = model(v_inputs)\n",
    "                v_loss = criterion(v_outputs, v_labels)\n",
    "                v_running_loss += v_loss.item()\n",
    "\n",
    "            print(\"Time: \", datetime.datetime.now().strftime(\"%H:%M:%S\"), \"\\tepoch: \", epoch+1, \"batch: \", i+1, \"Training loss: \", running_loss, \"Validation loss \", v_running_loss)\n",
    "            validation_losses.append(v_running_loss)\n",
    "            training_losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "        \n",
    "\n",
    "        # once the validation has been completed, update the model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # set model back to training mode\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "train_time = end_time - start_time\n",
    "print(\"Elapsed Training Time: \", datetime.timedelta(seconds = train_time))\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1241b0fc10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(type(outputs))\n",
    "#print(outputs.size())\n",
    "#print(outputs[1, :])\n",
    "#print(labels.size())\n",
    "#print(labels[1, :])\n",
    "#print(outputs.dtype)\n",
    "#print(labels.dtype)\n",
    "#print(loss)\n",
    "\n",
    "#print(training_losses)\n",
    "\n",
    "plt.title(\"Train and Val Loss\")\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_496121/1705838127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# put the model back in evaluation mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mincorrect_combination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# now that the training has been complete, let's run through this baby with our test set.\n",
    "\n",
    "# put the model back in evaluation mode. \n",
    "model.eval()\n",
    "incorrect_combination = []\n",
    "\n",
    "# now run the validation set once more to pull out the mis-typed images. \n",
    "\n",
    "incorrect_classification = np.ndarray((1,14))\n",
    "#incorrect_label = np.ndarray(1)\n",
    "\n",
    "for v, vdata in enumerate(val_loader):\n",
    "    vinputs, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "    voutputs = model(vinputs)\n",
    "\n",
    "    for i in range(voutputs.shape[0]):\n",
    "        for j in range(voutputs.shape[1]):\n",
    "            if voutputs[i,j].cpu().detach().numpy() != vlabels[i,j].cpu().detach().numpy():\n",
    "                #predictedClass = int(voutputs[j].cpu().numpy())\n",
    "                #actualClass = int(vlabels[j].cpu().numpy())\n",
    "                #myTuple = (predictedClass, actualClass)\n",
    "                myTuple = (int(voutputs[i,j].cpu().detach().numpy()), int(vlabels[i,j].cpu().detach().numpy()))\n",
    "                incorrect_combination.append(myTuple)\n",
    "            incorrect_classification = np.append(incorrect_classification, incorrect_combination)\n",
    "            # incorrect_label = np.append(incorrect_label, vlabels[i].cpu().numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Counter from the collections module to count the occurrences of each item\n",
    "counter = collections.Counter(incorrect_combination)\n",
    "\n",
    "# Find the most common item and its count\n",
    "most_common_item, count = counter.most_common(1)[0]\n",
    "\n",
    "print(f\"The most common misclassification occured: {count} times\")\n",
    "print(f\"Categorized: {catagories[most_common_item[1]]}\")\n",
    "print(f\"As: {catagories[most_common_item[0]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
